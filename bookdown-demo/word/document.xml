<?xml version="1.0" encoding="UTF-8"?>
<w:document xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" xmlns:pic="http://schemas.openxmlformats.org/drawingml/2006/picture" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing"><w:body><w:p><w:r><w:pict><v:rect style="width:0;height:1.5pt" o:hralign="center" o:hrstd="t" o:hr="t" /></w:pict></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">title:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">GESC 151: Our Digital Earth</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">author:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">Colin Robertson</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">date:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">2020-11-13</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">site: bookdown::bookdown_site</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">output: bookdown::word_document2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">documentclass: book</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">bibliography: [book.bib, packages.bib]</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">biblio-style: apalike</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">link-citations: yes</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">always_allow_html: true</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">description:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">Aadaptation of demo project for GESC 151 course development.</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r></w:p><w:p><w:r><w:pict><v:rect style="width:0;height:1.5pt" o:hralign="center" o:hrstd="t" o:hr="t" /></w:pict></w:r></w:p><w:bookmarkStart w:id="24" w:name="preface" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">Preface</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This is a document in support of the GESC 151 Our Digital Earth course.</w:t></w:r></w:p><w:bookmarkStart w:id="20" w:name="course-goal-and-learning-outcomes" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Course Goal and Learning Outcomes</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The goal of this course is to develop an understanding of how the earth is represented in digital systems and how these representations can be used to address environmental issues of societal relevance. In completing this course, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe how information about the earth is captured and represented in digital systems</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explain how a geographic perspective on the world can contribute to understanding both natural and anthropogenic processes</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Learn how to access and analyze geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Utilize freely available geospatial technologies to map spatial distributions of geographic features and processes</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1001" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Assess the accuracy and limitations of digital systems and geospatial data</w:t></w:r></w:p><w:bookmarkEnd w:id="20" /><w:bookmarkStart w:id="21" w:name="case-studies-used-in-this-course" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Case studies used in this course</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">As DE tools and technologies can be used in so many different types of applications, we have selected a handful of sample problems which we will use to illustrate key concepts and tools in the course. The case studies as follows</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Crime and policing in the City of Toronto/Waterloo Region</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Mapping the weather</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">COVID-19; mapping and modelling</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Mapping temperature extremes</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Personal and community geographic knowledge</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Tracking the Sahara Desert cloud</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1002" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Global freshwater ecosystems</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">These case studies cover mapping, satellite imaging, big data and surveillance, climate change, among many other topical issues that intersect with the environment and its digital representation. As we move through the course, we encourage to pause often to think about connections between the case studies, and how their digital representation effects how we come to understand and manage these important issues.</w:t></w:r></w:p><w:bookmarkEnd w:id="21" /><w:bookmarkStart w:id="22" w:name="course-readings" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Course readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">There is not a required textbook for this course.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Weekly readings and related materials will be posted to the course website and integrated within the course materials.</w:t></w:r></w:p><w:bookmarkEnd w:id="22" /><w:bookmarkStart w:id="23" w:name="how-this-course-is-structured" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">How this course is structured</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This course is comprised of 8 content modules, spread out over 10 weeks. Following that we have an exam, a term project and a town hall forum where we can showcase our projects.</w:t></w:r></w:p><w:bookmarkEnd w:id="23" /><w:bookmarkEnd w:id="24" /><w:bookmarkStart w:id="48" w:name="intro" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">Introduction to the Digital Earth</w:t></w:r></w:p><w:bookmarkStart w:id="29" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="27" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1003" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId25"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Al Gore’s Digital Earth Speech</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1003" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId26"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Satellite-based earth observation article in Scientific American</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="27" /><w:bookmarkStart w:id="28" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Cite several examples of how geospatial data are used in everyday life</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explain the concept of the digital earth</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Provide an overview of how the world is represented in digital tools</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1004" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe what it means for data to be considered geospatial</w:t></w:r></w:p><w:bookmarkEnd w:id="28" /><w:bookmarkEnd w:id="29" /><w:bookmarkStart w:id="32" w:name="digital-earth-today" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Digital Earth Today</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">When you think about the big environmental issues our time: climate change, loss of biodiversity, deforestation, we often think of vivid visual imagery, of polar bears on ice drifts, or recently harvested tracts of Amazon forest. Rarely do we think of the following image:</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">## PhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.</w:t></w:r></w:p><w:bookmarkStart w:id="30" w:name="yfrFoj7Qwz" /><w:bookmarkEnd w:id="30" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Figure 1: A possible digital earth, vegetation anomalies as sensed by satellite sensors, visualization credit: B. W. Lewis</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Yes most research, management, and decision-making around these big problems rely on digital representations of the environment. In this course we explore how the concept of the digital earth is shaping how we come to understand and solve these critical issues.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The world is rapidly changing. When we consider how we have used maps and mapping in the past to record locations, routes, forest boundaries, roads, etc. we tended to depend on static versions of the world; singular snapshots in time captured on a paper map, or later, on digital version of a paper map. However this situation has changed dramatically. Almost every aspects of our lives are mediated through some digital technologies, and often these are encoded with some geographic details about the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">location</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">where things occur. Images and videos are increasingly obtained from drones, satellites, and mobile phones.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Consider a typical day for a university student in Canada, and the digital representation of this day.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="" /></w:pPr><w:r><w:t xml:space="preserve">Table 1:</w:t></w:r></w:p><w:tbl xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" xmlns:pic="http://schemas.openxmlformats.org/drawingml/2006/picture"><w:tblPr>NA&quot;/&gt;<w:tblLayout w:type="fixed" /><w:jc w:val="center" /><w:tblW w:type="dxa" w:w="8640" /><w:tblLook w:firstRow="1" w:lastRow="0" w:firstColumn="0" w:lastColumn="0" w:noHBand="0" w:noVBand="1" /></w:tblPr><w:tblGrid><w:gridCol w:w="4320" /><w:gridCol w:w="4320" /></w:tblGrid><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /><w:tblHeader /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:b /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Activity</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:b /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Digital</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Wake up</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Phone records first time you check your messages, has location reference connected with Wifi router at home</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Catch the bus to school</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Swiping bus pass records your ID and bus stop location and time you board</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Attend classes</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Using student card and payment card records time and location of various activities throughout the day, connecting to different wifi spots as you move about campus records location</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Meet friends for coffee</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Using bank card for purchase records time and location</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Go to volleyball practice</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Cell phone pings of towers recording approximate location as you move around the city</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Go home</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Netflix connection records device ID and time you log on</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">All day long</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Satellite imaging of locations you are at various resolutions, CCTV cameras capturing images and video, other people capturing you in photos and video</w:t></w:r></w:p></w:tc></w:tr></w:tbl><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The number of digital interactions we engage in increases as more and more of our lives are mediated through digital devices networked through the Internet. An increasing share of these interactions include some level of geographic information about</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">where</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">on the earth they occur. So much data is being generated about the earth and what we do in it, the idea of the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">digital earth</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">has been increasing in popularity.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">While we can immediately see how our digital lives can be encoded to digital data, it is perhaps less obvious how our understanding of the environment is dependent on geospatial technologies and the digital earth. Climate change, extreme weather, flood forecasting and flood mapping, and more mundane activities such as management of streetlights, land parcel boundaries, and road construction all rely on an accurate digital record. The key defining feature about this type of digital record, often called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">geospatial data</w:t></w:r><w:r><w:t xml:space="preserve">, is that the data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">knows</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">where it is, relative to other things. For example, a digital record of a new road, needs to also know about the slope and elevation, subsurface materials, existence of other connecting roads, land parcel boundaries, right-of-ways, etc. And each of these also needs to be aware of their relationship to many other features and characteristics of the world we live in.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The way that geospatial data accomplishes this</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">spatial awareness</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is by referencing location relative to the entire earth using a coordinate system. All we need to know about coordinate systems right now is that they are a way of marking position in space, and they can be tied to the surface of the earth, so that positions on the earth can be located accurately. We therefore need a model for the earth upon which to construct a coordinate system. There are many models of the earth and ways to create coordinate systems, but in order for data to be considered geospatial, they must be encoded with this special information that allows it to be accurately located on the earth.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:hyperlink r:id="rId31"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read about what geospatial data is here</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="32" /><w:bookmarkStart w:id="36" w:name="whatis" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">What is the Digital Earth</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The term</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">Digital Earth</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">was coined by US Vice President Al Gore in 1998 in an address at the California Science Center.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId25"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">You can read the full text of his speech here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. The vision lays out an ambitious idea to create a 3D version of the planet that has historical and real time data on weather, traffic, crime, housing and a limitless array of other characteristics. You may be thinking that this sounds a bit like Google Earth; and indeed the first version of Google Earth was released just a few years after this call to action. Although this vision for a Digital Earth was laid out over 20 years ago; the key points remain important today, a Digital Earth must have</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1005" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">an explorable interface for navigating the earth</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1005" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">geospatial data from a vast array of sources</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Google Earth is a great introduction to the Digital Earth, here is a short clip (0:06) showing an urban environment. Think about how the buildings, roads and trees in this clip are digital versions of the real objects. Sometimes each building or tree is stored as an object with its own information, while other times they are encoded as pixels within a image. The way that real world objects and phenomena are stored in their digital forms has a big impact on how you can use them.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Compare navigation in Google Earth shown above to the Urban Observatory web application</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId33"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">which you can explore here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. The Urban Observatory allows you to compare different</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">themes</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">across three cities simultaneously. This is much closer to the ideas outlined in Al Gore’s 1998 address.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2373068" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 2: Comparing real time wind data between New York, Tokyo, and London with the Urban Observatory" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/ua.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId34" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2373068" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 2: Comparing real time wind data between New York, Tokyo, and London with the Urban Observatory</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Try comparing the Housing Density theme across New York, Tokyo, and London.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1006" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">What patterns do you see?</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1006" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">What questions does seeing the geospatial data displayed in this way evoke?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Spend some more time exploring the Urban Observatory application to get a feel for viewing and comparing patterns in geospatial data. These are key skills for working with Digital Earth tools.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth vision articulated in 1998 was not the result of a new invention. Rather, the idea was based on the maturing of several technologies that had been (and continue to be) developed and increasing in power and capability. We will briefly define some of the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">core building blocks on the Digital Earth</w:t></w:r><w:r><w:t xml:space="preserve">:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1007" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Geographic Information Systems</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(aka GIS) - can be defined as an information system tasked with representing geographic objects, relationships, and processes</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(from</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">???</w:t></w:r><w:r><w:t xml:space="preserve">)</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1007" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Satellite-based Earth Observation</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(a form of Remote Sensing) - has been increasing for the past 50 years, and we now have a rich archive of satellite images capturing changes to earth’s environment at a variety of spatial and temporal scales. Satellite imaging provides much of the data needed for the Digital Earth vision to be realized.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId26"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read more about satellite-based earth observation in this article in Scientific American</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1007" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Global Navigation Satellite Systems</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(aka GNSS, of which GPS is one type) - these are satellite-based systems which - among other things - allow positioning information on earth to be determined based on a receivers distance from several satellites it connects to. Originally developed as a military technology by the US Government, GNSS and the most widely used variant the Global Positioning System (GPS) is now vital to everything from getting directions on your smart phone to mapping polar bear sightings.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId35"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read more about GPS and GNSS here</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1007" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Geographic Information Science</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(aka GIScience) - you may be thinking this sounds a lot like #1 above, and it is. However the field of GIScience has built up over the past four decades to provide a theoretical basis for the tools and technologies noted above. A good way to distinguish GIS from GIScience, is that if it is about a software tool it is GIS, and if it is about a concept, theory, or more generic methodology, it can be described as GIScience. This distinction in nomenclature is not super important, but serves to highlight that there is a deeper body of work supporting the development of the Digital Earth, and one which continues to be developed by scholars in a wide range of disciplines.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Many other fields also play important roles in building the Digital Earth: geodesy, surveying, cartography among others; and many other areas of social and natural science provide the application contexts which drive the development of tools and technologies further. Classic users of these tools have come from forestry, agriculture, landuse planning, mining, conservation, and transportation. You may notice a similarity here; fields focused on land-based issues are highly dependent on geospatial data and the technologies that make use of geospatial data.</w:t></w:r></w:p><w:bookmarkEnd w:id="36" /><w:bookmarkStart w:id="37" w:name="physical" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">From the Physical to the Digital Earth</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">A key thing to remember is that geospatial data in the Digital Earth and the actual variables and processes those data represent are different. The wind data shown in Figure above is wind as recorded on a specific date and time. As well, the wind is measured using an anemometer at a weather station, which are located at specific locations. Often, multiple measurements from different weather stations are combined to generate maps of the dominant wind direction and speeds. However - since the actual measurements are only made at the stations, everywhere else is an estimate, which may ignore how wind interacts with the terrain, buildings, etc., and thus is likely not accurate everwhere. A critical skill of working with geospatial data is to think about where the data comes from, how accurate it is, how it changes over time, and where and why it might misrepresent reality. As we will see this is especially important when working with datasets describing human dimensions and societal conditions.</w:t></w:r></w:p><w:bookmarkEnd w:id="37" /><w:bookmarkStart w:id="44" w:name="themes" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Themes, Layers, and Abstractions of Reality</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In the Urban Observatory application noted above, a key idea is the representation of features and characteristics of the environment as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">layers</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">that can be toggled on and off. This idea has its roots in actual layers of acetate, whereby different</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">types</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of information: rivers, contours, cabins, hiking trails, etc. were mapped on separate plastic sheets which could then be viewed on a light table, and removed or added as needed. This gave much more flexibility in seeing how features related to each other. Watch the following short clip (~3 mins) from long ago describing the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId38"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Canadian Geographic Information System</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is often cited as the first GIS ever created - a system designed to manage the large land resources of Canada.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">You can see from the above, that having to deal with transparancies with maps printed on them would not be an efficient way to manage information about the environment, even in the 1960s. GIS technology was created to use the power of computers to solve this</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">big data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">problem. In GIS today, as shown in the Urban Observatory above, the idea of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">thematic layers</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">remains a very important idea. Actually, as we will see - a variety of non-digital map-based ideas were simply made into digital equivalents. Even the latest GIS technologies and mapping tools make use of layers, coordinate systems, datums, and others concepts and ideas that have been around for hundreds of years.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Thematic layers of geospatial data represent the geographic distribution of a single type of phenomena, in addition to potentially many other descriptive attributes of that phenomena. There are two fundamental ways layers capture information about a characteristic in the envrionment we want to represent in the Digital Earth: as a discrete object using something called the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">object data model</w:t></w:r><w:r><w:t xml:space="preserve">, or as a continuously varying function of space, using something called the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">field data model</w:t></w:r><w:r><w:t xml:space="preserve">. We will get into these data models in more detail later, but intuitively we can think of how we might map different types of phenomena. If we wanted to record locations of where bicycle accidents occured in our city, we would probably want to use the specific location - represented as a point on a map.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId39"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">This is exactly what you can do in the BikeMaps.org citizen science project</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, which uses web-based mapping to record and display bike collisions, thefts, hazards, and near-misses.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3763617" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 3: Citizen science project Bike Maps uses web-mapping and geospatial data to track bicycle collisions, near misses, and thefts in cities across the world. Here the orange pin has been clicked to explore more attribute information about the near-miss event." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/bm.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId40" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3763617" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 3: Citizen science project Bike Maps uses web-mapping and geospatial data to track bicycle collisions, near misses, and thefts in cities across the world. Here the orange pin has been clicked to explore more attribute information about the near-miss event.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Take some time to explore the BikeMaps site and see how it works, if you ride a bike and have a collision or near-miss to report, make a submission! These sorts of online tools get their power by the collective contributions from people just like you (citizen science!) and can lead to insights not otherwise possible. Notice that when you click on a pin, extra information about that event comes up (these are called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">attributes</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in GIS terminology).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Bicycle collisions, lend themselves very well to a point-based representation on a map; the happen at specific locations, at specific points in time. You can imagine that by mapping these events over time for a given city (as BikeMaps is aiming to do across the world), you get a sense of which intersections have a higher risk of collision, and steps can be taken to improve safety at these locations. This is the sort of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">spatial pattern</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">analysis that can really only be done through the collection and analysis of geospatial data. We might also want to know the cause of an area of high risk; for example whether its due to heavy road traffic, or the physical design/visibility of the bikelane, or a host of other possible explanations. Now if these factors were also geospatial, we could evaluate which cause is most likely, and then confirm this through investigating more details of reported collisions, and ultimately come up with a solution to reduce the number of collisions by modifying the environment.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The second data model for representing data as geospatial, is called the field data model, and this is for phenomena that varies continuously across space. What does this mean exactly? It’s best to think of some examples: air temperature, surface elevation, landcover; each of these is something that can be</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">measured</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">anywhere on earth. If we wanted to map waterloo air temperature, we could have an infinite number of points. Sine this is not possible, the most common was to represent this type of continuous spatial phenomena is as a grid of pixel elements, otherwise known as pixels. Each pixel knows its location on the earth, and the value attached to each pixel is a measurement of the phenomena of interest (e.g., temperature). Landcover mapping is another example; whereby any location can be ascribed a landcover class, such as forest, urban, agriculture, etc.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId41"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Here is an example of a landcover map for all of Canada</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Take some time to explore panning and zooming around this</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">webmap</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">To really understand what we’re seeing here, we would need to look at the map legend that associates the colour of each pixel to a specific landcover class. However, we might be able to figure out what some colours are. For example, can you figure out what the light brown colour represents? What about red?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Note also that when we click on this map, we do not get any extra information. Each pixel only knows what its landcover is - nothing more. This landcover map is the result of a classification of satellite imagery. Since this is a mostly automated process, it can be replicated each year, and in this way can track the loss or gain of specific landcover types. Think about how or why this might be useful.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3239159" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 4: Canada landcover map, 2015, derived from satellite imagery. Source: Natural Resources Canada. Screenshot taken by Colin Robertson. Used wit permission." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/clc.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId42" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3239159" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 4: Canada landcover map, 2015, derived from satellite imagery. Source: Natural Resources Canada. Screenshot taken by Colin Robertson. Used wit permission.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:hyperlink r:id="rId43"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Now, take some time to read about how this geospatial data was created and what the colours mean</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. This is a very important geospatial dataset that can serve a wide variety of applications.</w:t></w:r></w:p><w:bookmarkEnd w:id="44" /><w:bookmarkStart w:id="45" w:name="career" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth Career Path</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">As the Digital Earth includes a wide variety of tools, datasets, and fields where it can be applied, there are a number of career paths related to the Digital Earth. Classic job titles that use the Digital Earth include GIS jobs, such as GIS Analyst or GIS technician; often withing government, resource management, or environment and conservation sectors of the economy. As Digital Earth tools become more accessible and pervasive, new opportunities and career paths will open up at the interface of these areas in specific sectors. More recently, data scientist and spatial data scientist jobs are emerging, encompassing a wider variety of skills that have to do with processing, analyzing, visualizing, and making sense of geospatial data in order to improve decision-making. This includes some skills in statistics, programming, cloud computing, machine learning, and project management.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Many career paths in physical geography, human geography and social sciences, ecology, etc. are increasingly dependent on having</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">some</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Digital Earth skills and experience. This sort of career path will be different from</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">GIS Experience</w:t></w:r><w:r><w:t xml:space="preserve">, but include the full suite of interacting technologies that comprise the Digital Earth. As more and more data becomes accesible and part of Digital Earth systems, and more integrated and interconnected, new career paths will open up at these critical junctures, for example geographic information and artificial intelligence, scenarion modelling and environmental forecasting, and public and stakeholder engagement with geovisualization tools.</w:t></w:r></w:p><w:bookmarkEnd w:id="45" /><w:bookmarkStart w:id="47" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">At this point, it is worth revisiting some key ideas we have covered thus far. Firstly, the Digital Earth is not a single tool or technology, but an expansive vision of a system for managing every aspect of the environment, and the natural and social worlds which comprise it. This system is dependent on a variety of core technologies which have matured over the past 40 years, but are ever evolving and improving. Secondly, geospatial data; data that are anchored to locations on the earth, are crucial to the Digital Earth. Only when individual datasets can know their relation to other datasets does the power of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">spatial analysis</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">become apparent. The utility of geospatial datasets increases as more and more data are geographically referenced to the same model of the earth. Finally, the Digital Earth is comprised of representations of reality; often modelled as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">thematic layers</w:t></w:r><w:r><w:t xml:space="preserve">. There are different data models for abstracting the real world into the Digital Earth - and decisions around this abstraction process have important implications for what we can do with the data. We will be delving into many of the topics covered in this introductory lesson in further detail in the coming weeks.</w:t></w:r></w:p><w:bookmarkStart w:id="46" w:name="key-terms-from-this-week" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key terms from this week</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Geographic Information Systems</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Satellite-based Earth Observation</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Global Navigation Satellite Systems</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Geographic Information Science</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">thematic layers</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">object data model</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">field data model</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1008" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">webmap</w:t></w:r></w:p><w:bookmarkEnd w:id="46" /><w:bookmarkEnd w:id="47" /><w:bookmarkEnd w:id="48" /><w:bookmarkStart w:id="89" w:name="action" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth in Action</w:t></w:r></w:p><w:bookmarkStart w:id="52" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="50" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1009" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId49"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Paper using satellite data to monitor water storage</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="50" /><w:bookmarkStart w:id="51" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1010" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explain how different geospatial datasets can be brought together around a common theme</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1010" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Distinguish single-purpose and multi-purpose geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1010" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Find examples of geospatial applications</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1010" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe how issues of data quality and bias can impact geospatial data and applications</w:t></w:r></w:p><w:bookmarkEnd w:id="51" /><w:bookmarkEnd w:id="52" /><w:bookmarkStart w:id="62" w:name="the-digital-earth-is-already-here" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth is Already Here</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth is part of many aspects of everyday life. As illustrated in week 1, as more and more of our lives are digitized, more data is collected, stored, analyzed, and used to drive decision-making. You often hear</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">evidence-based</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">data-driven</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which often mean some data has been collected and is being analyzed to reveal insights and inform policy, guidelines, or notions about how we should/can act in the world. This applies to our personal lives, as well as our interactions with and management of natural resources, wildlife, public services, and many other facets of life. The so called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">data-deluge</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">has roots in almost all industries and sectors. As a result,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">Data Scientists</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">has been among the most rapidly growing job titles for the past several years.</w:t></w:r></w:p><w:bookmarkStart w:id="61" w:name="covid-19-mapping-the-pandemic" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">COVID-19: Mapping the Pandemic</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">You are taking this course, either currently living through or having just experienced the greatest public health crisis the world has seen in over a century. COVID-19 is a truly global scale phenomena. A new virus emerges, has a unique combination of characteristics that make it spread easily. As it is new - no one has immunity, nor is there any baseline knowledge about risk factors, long-term impacts, etc. As such, the response to COVID-19 has been reactive and evolving, as more knowledge is generated about the characteristics of the virus and the disease it causes.</w:t></w:r></w:p><w:bookmarkStart w:id="60" w:name="maps-and-disease" /><w:p><w:pPr><w:pStyle w:val="Heading4" /></w:pPr><w:r><w:t xml:space="preserve">Maps and Disease</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Mapping has been an integral part of studying epidemics throughout history. Because the trajectory of an infectious disease epidemic often depends on how humans interact, putting cases of disease</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">on the map</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">can provide insights into sites or mechanisms of transmission or sub-populations at greater risk. There is an entire field of geography and statistical analysis devoted to disease mapping, disease cluster detection and understanding of geospatial risk factors of health and disease. In the case of COVID-19, you have likely seen the dashboards that show cases such as the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId53"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">John Hopkins University COVID-19 Dashboard</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, or the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId54"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Ontario COVID-19 Data Tool</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. These tools typically allow you to quickly see where cases are high or low and how they have evolved over time. As we all know firsthand, these sorts of tools become instrumental in the experience of the pandemic and impact behaviours, social dynamics, and ultimately disease transmission itself.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Consider something as simple as the case count, for example for Waterloo Region as of September 12 2020. According to official data, the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId55"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">total cumulative case count of COVID-19 cases for this data was 1518 positive cases</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Try to think of some questions about the data-generating process that gave rise to this number. What critical questions could you ask?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BlockText" /></w:pPr><w:r><w:t xml:space="preserve">Once you have a few questions click the button below.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Show/ hide</w:t></w:r></w:p><w:bookmarkStart w:id="56" w:name="myDIV" /><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Are these cases based on home address or place where test was done or some other geographic criteria? Some groups such as students may have a home address located in another area, but currently reside in Waterloo Region.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">How many tests that are positive are still active?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">How accurate is the test? What is the false positive and false negative rate?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">What are the rules for who can get tested? Can asymptomatic people get tested or only those with symptoms or with a known exposure to a case?</w:t></w:r></w:p><w:bookmarkEnd w:id="56" /><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Such questions underpin the linkage between the case count (i.e., the number in the data) and the true number of infections within the Region.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Too often when information is presented as</w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /><w:i /></w:rPr><w:t xml:space="preserve">data</w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">key questions about what processes actually led to the number in the data are absent.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In the early phase of the pandemic in the US, only those with travel history to Wuhan (i.e., the origin of the virus) or those who had come into contact with a known infected person qualified to be tested. This narrow definition had the effect of keeping the case count low and thus it could be said it was</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">under control</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve">. Looking back we see the low counts were only because of the restrictive requirements for testing.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BlockText" /></w:pPr><w:r><w:t xml:space="preserve">The key lesson here is to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">always scrutinize data</w:t></w:r><w:r><w:t xml:space="preserve">, always</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">be critical and ask questions</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">about what decisions and processes went into creating the data - in short: what is the nature of the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">data generating process</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">With that in mind -</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId57"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">take a look at this</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /><w:b /></w:rPr><w:t xml:space="preserve">geospatial visualization</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">of COVID-19 which allows you to start and stop an animation of the global geographic spread.</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2779346" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 5: Visualization of spread of COVID-19, source: https://www.nbcnewyork.com/ Screenshot taken by Colin Robertson. Used with permission" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/coronoa.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId58" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2779346" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 5: Visualization of spread of COVID-19, source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId59"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://www.nbcnewyork.com/</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Screenshot taken by Colin Robertson. Used with permission</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Examine the geographic spread of the virus using the animation tool linked above. Think critically about what you are seeing.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Write down three questions</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">that you could ask to interrogate the data further.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkEnd w:id="60" /><w:bookmarkEnd w:id="61" /><w:bookmarkEnd w:id="62" /><w:bookmarkStart w:id="71" w:name="X49d84eaa1932b388d0863c5a14fe18168f981f0" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Critically Thinking about Geospatial Data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Any kind of data that can be digitally represented as a geographic distribution (i.e., a map!) must be examined just as critically as the public health data examined above. Geospatial data are the data that underly maps, as well as a whole variety of software programs, apps, web services, etc. Just as we can ask critical questions of the COVID-19 data above, we need to ask questions about geospatial data.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="" /></w:pPr><w:r><w:t xml:space="preserve">Table 2:</w:t></w:r></w:p><w:tbl xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" xmlns:pic="http://schemas.openxmlformats.org/drawingml/2006/picture"><w:tblPr>NA&quot;/&gt;<w:tblLayout w:type="fixed" /><w:jc w:val="center" /><w:tblW w:type="dxa" w:w="8640" /><w:tblLook w:firstRow="1" w:lastRow="0" w:firstColumn="0" w:lastColumn="0" w:noHBand="0" w:noVBand="1" /></w:tblPr><w:tblGrid><w:gridCol w:w="4320" /><w:gridCol w:w="4320" /></w:tblGrid><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /><w:tblHeader /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:b /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Questions</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:b /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Answers</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">How were the data collected?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Satellite sensor, digitized, copied from existing geospatial data, GPS, surveying, hand drawn</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Why was the data collected? For what purpose?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">For regular monitoring, as part of a specific study, for commercial purposes, etc.</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Are the data &#39;open&#39; [i.e., free and publicly available]- If not, why?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Yes, No, not sure, its commercial data, privacy reasons, etc.</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">How were the data processed?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Converted from one format to another, reprojected to a different coordinate system, new derived attributes were computed</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Who is the audience for this map/visualization/application?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">The general public, students, people using public transportation, senior citizens, the military, police forces</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">How current are the data?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Today, last year, unknown, last updated June 1 2017, etc.</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">What or who is misssing from the data?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">People without a fixed address, people without cell phones, people that live in apartment complexes, rural areas, etc.</w:t></w:r></w:p></w:tc></w:tr></w:tbl><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Answering these types of questions takes time and effort. Sometimes geospatial data are purpose-built; meaning they are collected specifically for one purpose and are confined to use in that specific scope. This is what we can call</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">single-purpose geospatial data</w:t></w:r><w:r><w:t xml:space="preserve">. Consider for example, a property survey of a suburban house. This data could be stored as a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">vector geospatial dataset</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">- essentially a collection of points, lines, and polygons encoded with geographic information and additional descriptive information. These data will typically serve no other purpose than to provide a digital record for where the property boundaries are located. However, increasingly, geospatial data are applied in a variety of uses. These</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">multi-purpose geospatial data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">can serve a seemingly endless number of applications. Some forms of multi-purpose geospatial data play a role in a wide-number of use-cases, such as a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">digital elevation model</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(DEM) (which records elevation data) or a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">land-cover dataset</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which we saw in Week 1. These data are multi-purpose because they serve as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">basemap data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and many other applications depend on them. For example, DEM data might be used to detect areas at risk of avalanche, to describe the habitat charcteristics where observations of an endangered species have been made, or in evaluating locations suitable for a new industrial facility. Other examples of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">basemap geospatial data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">include roads, water bodies, building footprints, and aerial imagery.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Think - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Basemap geospatial data is crucially important to the Digital Earth as these datasets fuel many applications and services. However, there is no agreed-upon standard or source for these essential datasets, rather they are produced by all levels of government, at different levels of detail, using different methods and coordinate systems, and have widely varying degrees of access (i.e., free and open source to commercial data costing thousands of dollars).</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Think about why this could cause some problems.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Understanding how to access, evaluate, and integrate appropriate basemap data is a key skill!</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Another form of multi-purpose geospatial data is an emerging type of data that are used for completely different purposes than it was collected for. This type of data is more common in what is typically referred to as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Big Data</w:t></w:r><w:r><w:t xml:space="preserve">. The term</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Big Data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">- which we will explore in depth in a later week - is often desribed in terms of the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">three v’s</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">-</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">volume</w:t></w:r><w:r><w:t xml:space="preserve">,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">variety</w:t></w:r><w:r><w:t xml:space="preserve">, and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">velocity</w:t></w:r><w:r><w:t xml:space="preserve">. Essentially; data that is</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">large in size relative to standard data storage and processing tools</w:t></w:r><w:r><w:t xml:space="preserve">, data that are produced in a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">wide variety of formats</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and representations (e.g., tabular, image, free-text, video, etc.), and data that are</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">rapidly updated and continuously being collected</w:t></w:r><w:r><w:t xml:space="preserve">. To understand how big data intersects with multi-purpose geospatial data we can consider an example; that of mapping the global distribution of ships at sea.</w:t></w:r></w:p><w:bookmarkStart w:id="70" w:name="Xb1e5f59236b671039fa9810c6a8e89ba6e0bf24" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Big Data Example: Mapping Global Shipping Patterns</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3540311" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 6: Ship at sea. Image source: Marco Verch" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/shio.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId63" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3540311" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 6: Ship at sea. Image source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId64"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Marco Verch</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Ships at sea need to communicate with each other, and a system called the Automatic Identification System (AIS) was developed in the 1990s as a way for ships to communicate critical information such as their ship’s identifier, geographic location, speed, and intendend route. Transceivers placed on board ships could communicate this data to other nearby ships (i.e. ship-to-ship) via on-board transceivers or to land-based transceivers (i.e., ship-to-shore). The primary use of AIS in its initial phase was navigation, collision avoidance, and aid in navigation - as an add-on to existing on-board radar. At some point in the mid-2000s it was realized that the AIS signals could be detected from space-borne sensors on satellites. This has led to the growth of a constellation of satellite-AIS (S-AIS) sensors that detect AIS messages and thereby giving an indication of ship location and related metadata. Scaled globally, this means it may be possible to map ship location in real-time for the entire earth; this is an example of big geospatial data.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">One of the leading companies in S-AIS,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId65"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Exact Earth Corp.</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is actually based in Waterloo Region. See how they market their S-AIS data and how it relates to different</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">geospatial applications</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in the short video below. Pay attention to what problems and/or issues this one global geospatial dataset is being applied to.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">One of the often-touted benefits of big data is its potential for hidden insights into processes which would otherwise go unknown. The story of S-AIS and evolution of global-scale ship monitoring from space is a perfect example of new use-cases can emerge from big data. For example, researchers have used S-AIS data to detect illegal fishing and as part of maritime surveillance for piracy.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">There are many secondary web applications that make use of A-AIS data,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId66"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">for example Marinetraffic.com. Explore the web-map here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1011" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Click on a few ships and explore the rich attributes that are tracked for each vessel.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1011" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Try click on the Past Track button of a vessel to see where it has been.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1011" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explore some of the options in the user interface.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Gaining this synoptic scale view of marine vessels has many benefits and uses, as noted in the Exact Earth video above. Seeing the data in map form also tends to lead to questions about spatial distribution:</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Why are there lots of points here and none there?</w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Do bulk carriers use the same routes as non-commercial ships?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">* What regions have higher risks of collisions, etc.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">These questions can be answered through the techniques of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">spatial analysis</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Take some time to explore the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId67"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">marinetraffic.com website</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which also includes rich data on ports, ships, shipping lanes, and other related factors. Some of these data are contributed by users (i.e., crowdsourced) while others are culled from other existing datasets. This represents a great example of a Digital Earth application, utizliing extensive geospatial data around a common theme.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Write down one way these different types of data could be used together</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to help solve a problem or answer a question.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Take some time to explore the marinetraffic.com website which also includes rich data on ports, ships, shipping lanes, and other related factors. Some of these data are contributed by users (i.e., crowdsourced) while others are culled from other existing datasets. This represents a great example of a Digital Earth application, utizliing extensive geospatial data around a common theme.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2676805" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 7: Screenshot of webmap at global ship tracking website, source: https://www.marinetraffic.com/" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/mt.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId68" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2676805" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 7: Screenshot of webmap at global ship tracking website, source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId69"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://www.marinetraffic.com/</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">However, as we noted earlier it is also important to apply a critical lens to the claims made by such data vendors. We can now ask some of the key questions noted above to this S-AIS global dataset:</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="" /></w:pPr><w:r><w:t xml:space="preserve">Table 3:</w:t></w:r></w:p><w:tbl xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" xmlns:pic="http://schemas.openxmlformats.org/drawingml/2006/picture"><w:tblPr>NA&quot;/&gt;<w:tblLayout w:type="fixed" /><w:jc w:val="center" /><w:tblW w:type="dxa" w:w="8640" /><w:tblLook w:firstRow="1" w:lastRow="0" w:firstColumn="0" w:lastColumn="0" w:noHBand="0" w:noVBand="1" /></w:tblPr><w:tblGrid><w:gridCol w:w="4320" /><w:gridCol w:w="4320" /></w:tblGrid><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /><w:tblHeader /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:b /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Questions</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:b /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Answers</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">How were the data collected?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Satellite-borne sensor picking up AIS messages transmitted from ships at sea</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Why was the data collected? For what purpose?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">The AIS messages exist primarily for navigation purposes</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Are the data &#39;open&#39; [i.e., free and publicly available]- if not, why?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">These are commercial data, viewable publicly but must be purchased to access the data</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">How were the data processed?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">S-AIS data go through a series of data processing steps. The messages are located within general regions of the earth where the messages from many ships might be simultaneously detected. S-AIS system has to organize messages in order and transmit the message data (as 256 bit slots) down to a base station where it can be decoded and reformatted as tabular data. The exact details for how message data are processed are commercial proprietary information.</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Who is the audience for this map/visualization/application?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Answer in your notebook</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">How current are the data?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Answer in your notebook</w:t></w:r></w:p></w:tc></w:tr><w:tr><w:trPr><w:cantSplit /><w:trHeight w:val="360" w:hRule="auto" /></w:trPr><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">What or who could be missing from the data?</w:t></w:r></w:p></w:tc><w:tc><w:tcPr><w:tcBorders><w:bottom w:val="single" w:sz="8" w:space="0" w:color="333333" /><w:top w:val="single" w:sz="8" w:space="0" w:color="333333" /></w:tcBorders><w:shd w:val="clear" w:color="auto" w:fill="FFFFFF" /><w:tcMar><w:top w:w="0" w:type="dxa" /><w:bottom w:w="0" w:type="dxa" /><w:left w:w="0" w:type="dxa" /><w:right w:w="0" w:type="dxa" /></w:tcMar><w:vAlign w:val="center" /></w:tcPr><w:p><w:pPr><w:jc w:val="right" /><w:spacing w:after="40" w:before="40" /><w:ind w:firstLine="0" w:left="100" w:right="100" /></w:pPr><w:r xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml"><w:rPr><w:rFonts w:ascii="Arial" w:hAnsi="Arial" w:eastAsia="Arial" w:cs="Arial" /><w:sz w:val="22" /><w:szCs w:val="22" /><w:color w:val="111111" /></w:rPr><w:t xml:space="preserve">Answer in your notebook</w:t></w:r></w:p></w:tc></w:tr></w:tbl><w:bookmarkEnd w:id="70" /><w:bookmarkEnd w:id="71" /><w:bookmarkStart w:id="75" w:name="Xbdbaa3ec2bce96afa70aecf442ab4270dcc0ec0" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Weather Prediction in the Digital Earth Era</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Weather prediction is perhaps the most fundamental, immediately useful application of the Digital Earth. We depend on weather apps on our phones, on the Internet, and in the media to help plan our days and weeks. These are vital sources of information for everday life, as well as for critical industries like agriculture. What our simple apps and websites hide is the fact that weather forecasts are the outputs of complex computer models of atmospheric processes. Inherent in weather forecasts are questions of scale. When we talk about</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">scale</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in geography or environmental studies, we typically mean the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">level</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">at which a process is operating. Common scales might be generic terms like local, regional and global. In geospatial and Digital Earth applications, scale is typically described in terms of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">spatial grain</w:t></w:r><w:r><w:t xml:space="preserve">, or the smallest size object that can be resolved by a sensor. For example, a sensor that can measure up to a 5 m road width in a single pixel would have a nominal spatial resolution of 5 m. We can also categorize spatial grain size in terms of resolutions: low resolution, medium resolution, high resolution which map onto linear dimensions of pixel sizes in geospatial data, such as 25 km, 1 km, and 10 m respectively.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Weather forecasts are the result of computer models which combine physical models of ocean and atmosphere processes into different iterations of possible weather, in order to generate scenarios extending into the future for a desired amount of time. As we all know, the 1-3 day forecast is generally more accurate than the 16-day forecast, as there is less uncertainty the shorter the forecast period is. To highlight just how much work goes into our daily forecast - we can briefly review the Global Ensemble Prediction System, which encapsulates physical processes in the ocean and atmosphere variables such as temperature, precipitation, cloud cover, wind speed and direction, and humidity up to 16 days into the future. These processes are modelled on a global grid with a resolution of 0.35 degree (roughly 39 km at equator). This means each</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">pixel</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">grid cell</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">covers an area roughly 40 km x 40 km - a fairly large area. This also means that any weather variability within that 40 km x 40 km grid cell footprint is lost. This system include geospatial data on sea ice, sea surface temperature, surface and deep soil temperature and moisture, snow depth, snow</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">albedo and snow density, and many other related physical variables. These models are extremely complex, combining surface observations with models of physical processes and their uncertainties. Each dataset within these systems can be thought of as a hidden layer of the Digital Earth, working in the background to combine with other datasets and models to produce the weather forecasts we use everyday. We can take a deeper look at the data used by Environment Canada on</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId72"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">their Open Data site</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 3</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId73"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read through one of the tutorials</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">showing how to work with the Environment Canada in the open source GIS software package</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId74"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">QIS</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. This shows how you can actually work with geospatial data to answer new questions - rather than just looking at pre-generated results.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">What is meant by the term</w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /><w:b /></w:rPr><w:t xml:space="preserve">in situ observations</w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">OPTIONAL: If you are up for it, download and install QGIS and try to run through the tutorial.</w:t></w:r></w:p><w:bookmarkEnd w:id="75" /><w:bookmarkStart w:id="86" w:name="X915b0672b5cef8140e920db35018289c647b1c6" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth Basemaps - Resources to Explore</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Have a look at the following Digital Earth geospatial datasets. As you explore, try to think about what these might be used for, how the data is measured, and potential issues or sources of bias in using them for particular purposes.</w:t></w:r></w:p><w:bookmarkStart w:id="77" w:name="terrestrial-water-storage" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Terrestrial Water Storage</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:hyperlink r:id="rId76"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">NASA Gravity Recovery and Climate Experiment (GRACE) Data Analysis Tool (DAT)</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 4</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId49"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read a related paper</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">using the GRACE TWS data.</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Q.4 What is happening in Figure 1? What happened in Texas in 2011? How was this detected?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkEnd w:id="77" /><w:bookmarkStart w:id="79" w:name="snow" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Snow</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:hyperlink r:id="rId78"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">GlobeSnow</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">project for snow-water equivalent (i.e., volume of water stored in the snowpack)</w:t></w:r></w:p><w:bookmarkEnd w:id="79" /><w:bookmarkStart w:id="81" w:name="human-settlements" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Human Settlements</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:hyperlink r:id="rId80"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Night time lights</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="81" /><w:bookmarkStart w:id="83" w:name="disease-outbreaks" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Disease Outbreaks</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:hyperlink r:id="rId82"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Health Map</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="83" /><w:bookmarkStart w:id="85" w:name="economic-inequality" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Economic Inequality</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:hyperlink r:id="rId84"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">GDP per Capita</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="85" /><w:bookmarkEnd w:id="86" /><w:bookmarkStart w:id="88" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Our goal this week was to explore how the Digital Earth and its related tools and technologies are being used today. We have seen that in everything from weather forecasting to global ship monitoring, digital representations of how natural and social processes are distributed geographically are critical. We have also highlighted how the same geospatial datasets can be reused for a wide array of practical applications. A digital elevation model for example, can be used for many different studies and applications because surface elevation is such an important component of many different fields. We have also reviewed how we can ask critical questions of Digital Earth data, and to understand that behind simple outputs might be many different models and datasets with their own biases, subjective decisions, and data quality limitations. As such, to become critical consumers of the Digital Earth, we need to ask questions about data source, processing, scale, who and why data were created, and how current data are. Geospatial data can create compelling visualizations and tools that allow you to explore the Digital Earth (e.g., marine traffic providing a digital window into global shipping); but this can also obscure what is missing or mis-represented.</w:t></w:r></w:p><w:bookmarkStart w:id="87" w:name="key-terms-from-this-week" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key terms from this week</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">data generating process</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">single-purpose geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">multi-purpose geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">basemap geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">digital elevation model</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">big data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">scale</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1012" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">in-situ observations</w:t></w:r></w:p><w:bookmarkEnd w:id="87" /><w:bookmarkEnd w:id="88" /><w:bookmarkEnd w:id="89" /><w:bookmarkStart w:id="133" w:name="Xb6b6fb9f95b52d969da2f18222f64481abb7f79" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">The View from Above: Satellite Imagery for Earth Observation</w:t></w:r></w:p><w:bookmarkStart w:id="95" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="93" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1013" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId90"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">2018 Keynote on the Queryable Earth, 45 min video</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1013" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId91"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Spectral bands in Landsat images</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1013" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId92"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Blog post which compares high and moderate resolution imagery</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="93" /><w:bookmarkStart w:id="94" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1014" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">List advantages of earth observation from space</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1014" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Define spatial and temporal resolution associated with satellite image data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1014" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Understand what image classification is</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1014" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Provide two examples of geospatial applications using satellite imgery</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1014" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe how high resolution image data differs from other types of image data</w:t></w:r></w:p><w:bookmarkEnd w:id="94" /><w:bookmarkEnd w:id="95" /><w:bookmarkStart w:id="98" w:name="what-is-earth-observation" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">What is Earth Observation?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Earth observation (EO) is the processes and technologies for recording earth information from space. Typically EO is part of programs run by national space agencies such as the Canadian Space Agency (CSA) in Canada or the National Aeronautics and Space Administration (NASA) in the US. EO programs are designed to acquire information about the earth or the atmosphere and repeatedly sense regions of the planet to support the monitoring of change-over-time. EO is a subfield of the broader field of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">remote sensing</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which pertains to any kind of observation-from-a-distance, including drone-based image acquisition, aerial photography,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId96"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">even photography from kites</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">!</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId97"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">There is a third-year class devoted completely to remote sensing in GES department.</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">as part of our Geomatics Option.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">You actually already have some experience working with EO data. When you explored the landcover map in Week 1, that map was derived from EO data (from the Landsat sensor), and in fact was produced by Canada Centre for Mapping and Earth Observation. There is a difference in terminology we should highlight here. A</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">map</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is a visual depiction of a geographic variable or region of interest. EO does not automatically produce maps, rather EO technologies produce</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">image data</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Image data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is direct physical measurements obtained by a sensor. The most simple form of EO is analagous to taking a photograph from space, in which the sensor is sensing reflected visible light. The photo-taking device is the sensor, and in EO it is mounted on a platform, which us usually a satellite in a defined orbit.</w:t></w:r></w:p><w:bookmarkEnd w:id="98" /><w:bookmarkStart w:id="109" w:name="the-earth-observation-process" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">The Earth Observation Process</w:t></w:r></w:p><w:bookmarkStart w:id="102" w:name="satellites" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Satellites</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Satellite imaging of earth has been going on since late 1950’s with the launch of the first satellites by the USSR and USA. The advantages of earth observation from space as opposed to from cameras mountain on aircraft which had been (a continues to be) a dominant form of remote sensing became apparent immediately. Aerial photography as the basis for generating geographic base data for mapping requires a designated flight campaign in support of it. As such, aerial photography is expensive, periodic, and more variable. Satellite-based EO is extremely costly and can only really be undertaken by national governemnt agencies and large corporations, however – once a program is in place data access can range from free to thousands of dollars per image. More importantly, satellites are placed into fixed orbits and have sensors usually have regular schedules for acquiring images.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3556000" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 8: Three orbits used in earth observation satellites, NASA illustration by Robert Simmon, source: https://earthobservatory.nasa.gov/features/OrbitsCatalog" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/geo.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId99" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3556000" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 8: Three orbits used in earth observation satellites, NASA illustration by Robert Simmon, source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId100"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://earthobservatory.nasa.gov/features/OrbitsCatalog</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">There are two orbits used in EO, noted in the figure above. A sun-synchronous orbit is at an altitude of 705 km, where sensors on satellites track the sunlit side of the earth in order to track visible light reflected from the earth. For sun-synchronous orbits a key property is how long they take to revisit the same location on earth. This is important for land monitoring and can have a critical impact on what type of monitoring we can do with the image data generated by the sensor. This property is called the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">revisit time</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">temporal resolution</w:t></w:r><w:r><w:t xml:space="preserve">. Revisit times are related to the swath width of the sensor (i.e., how wide an individual scene is). For higher resolution sensors, they tend to have smaller swath widths and longer revisit times, whereas lower resolution sensors have larger swath widths and shorter revisit times.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">A geostationary orbit tracks the same position on earth as the earth revolves around its access. Geostationary orbits sit at altitudes of approximately 35,000 km. EO satellites that sense in the visible range of the spectrum are in sun synchronous orbits.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">How spatial and temporal resolution combined to determine which applications can be targetted by a given EO sensor is a critical issue. You - as the EO expert - must be aware of the all of the considerations in making the selection of appropriate imagery. A generic representation of this is provide in the Figure below. Try to make sense of this before moving on.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4092997" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 9: How different temporal and spatial resolutions common in earth observation relate to geospatial applications they can be used on (Source: [from lec notes need to find original]" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/resolution-app.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId101" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4092997" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 9: How different temporal and spatial resolutions common in earth observation relate to geospatial applications they can be used on (Source: [from lec notes need to find original]</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Select two applications and provide an explanation for why they are linked with the spatial and temporal resolutions that they are in the image above.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="102" /><w:bookmarkStart w:id="104" w:name="sensors" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Sensors</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The sensor component of the EO process is the technology on the satellite that actually records information from earth. Sensors can be either</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">active sensors</w:t></w:r><w:r><w:t xml:space="preserve">, sensors which emit some energy which is reflected back from earth and built up into an image, or they can be</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">passive sensors</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which record reflected and emmitted imagery from the earth. Sensors can be designed in a wide variety of formats and specifications. What exactly sensors record is light, otherwise known as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">electromagnetic radiation</w:t></w:r><w:r><w:t xml:space="preserve">, which can be characterized as waveform, denoted by its wavelength, or the distance between two successive wave peaks. When we see different colours of a rainbow: red, orange, yellow, green, blue, indigo, violet what we are seeing are defined as specific ranges of wavelengths of electromagnetic radiation. As such, the light recorded by sensors is categorized into three primary colours - red, green, and blue. That is, red light, green light, and blue light is recorded onto separately onto discrete bands of imagery. Importantly, these sensors can also record in other wavelength ranges of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">electromagnetic radiation</w:t></w:r><w:r><w:t xml:space="preserve">, such as near-infrared (larger than visible wavelengths) or microwave (larger than near-infrared wavelengths). Sensing in these parts of the spectrum allow us to see into parts of the earth that are invisible to the human eye. Near-infrared imaging has roots in military applications for detecting camoflauged installations that have different spectral characteristics in the near-infrared part of the spectrum than the surrounding vegetation. Modern military equipment and companies now developed camoflauges that are not sensitive to these sorts of spectral differences, as see on this promotional video.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Note how they specifically describe how their camoflauge products are effective against specific parts of the elelctromagnetic spectrum. Many EO technologies have roots and are still actively developed in and for military applications. For example, In-Q-Tel - a venture capital arm of the Central Intelligence Agency (and in partnership with the US National Geospatial-Intelligence Agency), bought into a small DE company called Keyhole in early 2003 which had created technology for streaming image data onto a digital globe. The investment resusistated the small firm, integrated its technology into military and intelligence operations of the 2003/2004 Iraq War, and led to lucrative government contracts. In late 2004 Keyhole was acquired by Google, complete with CIA/military personnel and contracts, and the DE tool was soon rebranded as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId103"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Google Earth</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:bookmarkEnd w:id="104" /><w:bookmarkStart w:id="106" w:name="data" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Earth information recorded by sensors is processed into image data files where the information is quantized into digital numbers in pixels that make up an image. The image data is a square made up of pixels, each pixel has a single digital number which represents the amount of reflected or emmitted electromagnetic radiation over the area of the earth covered by the pixel. The total area covered by a single image is called a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">scene</w:t></w:r><w:r><w:t xml:space="preserve">. For example, the longest-running EO program Landsat, has a scene size is 185 km by 185 km, where each individual pixel is 30 m by 30 m. A newer EO program launched by the European Space Agency (ESA), the Sentinel-2 Program has pixels that are arranged into image scenes that are processed into 10 km by 10 km areas. We can get a sense of the scale of information recorded by image pixels by visualizing image data at different zoom levels. Figure below shows four views of the same Sentinel-2 image taken near Kakisa, Northwest Territories. The viewing scale in</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is 1:100,000, which means one unit measured on the image is equal to 100,000 times that unit on the ground. As we adjust the viewing scale to 10,000 (b), 50,000 (c), and 2500 (d), two things happen. Firstly we can see the features of the landscape in greater detail. Secondly, the extent that is visible in the image decreases. This viewing scale is norally called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">cartographic scale</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in mapping, and the terminology can be confusing. In short remember the following rule:</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BlockText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">bigger number, smaller scale, and larger number bigger scale</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">so a 1:1,000,000 map would be a small scale map, whereas a 1:1000 map would be a large scale map. As we move from</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">thru to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">d</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">we are moving from a smaller scale to a larger scale. As we do we can see more detail about the actual pixels that make up the image data. Have a closer look at the data in the figure below, can you figure out what the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">spatial resolution</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of this dataset is? Recall that the spatial resolution is the size of the pixel, usually specified in meters or decimal degrees. One way to guess this is to look at a feature in the image (road width, river bank) and try to count the number of pixels it takes to cover that feature. Then divide the expected length of the feature by the number of pixels, and you have your (guesstimated) spatial resolution.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">It turns out that these Sentinel-2 pixels have a spatial resolution of 10 m. This means detecting objects that are smaller than 10 m, such as people or bicycles, is not possible. The spatial resolution of an image pre-determines what we can do in terms of analysis with that image. When selecting EO data for a given problem, it is crucial to select appropriate data in terms of spatial resolution.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3795823" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 10: Sentinel-2 satellite image over Kakisa, Northwest Territories, visualized at relative resolutions of a) 1:100,000, b) 1:10,000, c) 1:50,000, and d) 1:100,000" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/resolution.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId105" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3795823" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 10: Sentinel-2 satellite image over Kakisa, Northwest Territories, visualized at relative resolutions of a) 1:100,000, b) 1:10,000, c) 1:50,000, and d) 1:100,000</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Since these data are recorded as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">multi-spectral</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">image data, the image channels (also called bands) (i.e., discrete sections of the electromagnetic spectrum - in this case blue, green, red, and near-infared light) can be combined in different ways (they are also called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">spectral bands</w:t></w:r><w:r><w:t xml:space="preserve">). For example, a widely used band combination for monitoring vegetation is the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">normalized differenced vegetation index</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is derived from the difference in the red and near infrared bands. It turns out that these bands behave very differently in health vegetation vs. unhealth vegetation, so we can use this to map vegetation health over large regions and to monitor vegetation over time. This has many applications in agriculture, forestry, and conservation. Again, selecting a sensor with the appropriate spectral bands for your problem is critical.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId91"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Take a look and review the spectral bands in Landsat imagery</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Here is an overview of the Sentinel-2 program from its launch in 2015. List two geospatial applications that are noted that this program can contribute to, how?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkEnd w:id="106" /><w:bookmarkStart w:id="107" w:name="analysis" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Analysis</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Once data are sensed, transmitted and processed into image data, they need to be further analyzed into more useful information products. Because EO image data is so rich, the process of data analysis usually involves reducing the amount of information to more simple types of information we are used to dealing with using for decision-making. EO image data also lacks any abstractions we use in our conceptualiztion of the world around us. For example, the pixels in the Sentinel image above that are brightly coloured and are from surfaces in the village and road (buildings and gravel), do not know that they are part of those spatial objects (or semantic categories), rather they just record the reflected light characteristics of those surfaces. Thus in order to move toward the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">queryable earth</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">as noted in the video keynote lecture, we have to associate pixels with spatial objects or semantic categories in order to reason and make use of these rich data sources.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">One of the main ways to go from EO image data to richer DE data is to do what is called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">image classification</w:t></w:r><w:r><w:t xml:space="preserve">. In simple terms, image classification is a process of training an</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">algorithm</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(i.e., a set of instructions a computer program to carry out on the image) to associate pixels in image data with particular labels that those pixels are associated with on the ground. When we view the image in the Kakisa figure above, we naturally recognize which pixels are in the river, which are on land, which are on the road, etc. Let’s think a little more about how and why we make these associations.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1015" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Colour</w:t></w:r><w:r><w:t xml:space="preserve">: seeing variation in colours in the image tells us about different surface features. Green looks like vegetation and brown looks like sediment-laden water.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1015" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Shape</w:t></w:r><w:r><w:t xml:space="preserve">: the river coming in to Kakisa Lake on the right side of the image looks like a river primarily because of its shape, as a sinuos line-like set of pixels</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1015" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Context</w:t></w:r><w:r><w:t xml:space="preserve">: where pixels are located on the image provides a rich about of information about what features on the landscape they are part of. If we extracted only the brown pixels, we would get much of the pixels in the river and on the lake, but we would also get some of the pixels on land in the vegetated area. These are sedges or bogs which have the same approximate colour as the sediment-laden water, but because they are located within a complex of green pixels, and distance inland from the lakeshore, we immediately recognize them as part of the land and make an association to a landscape feature or landcover based on this.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">It turns out that when we view an image like this, the humain brain is doing a lot of complex perception and association tasks that we take for granted. The</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">context</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">component is probably the most important, however most traditional image classification focus only on colour (i.e., spectral characteristics of the pixels). Only recently have image classification algorithms started to incorporate shape and context into their approach to classifiying pixels as part of more meaningful spatial objects or landcover categories. Once pixels are classified in this way, they are</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">queryable</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and can be used to generate answers about environemntal change, urbanization, conservation, and related environmental issues noted in the video.</w:t></w:r></w:p><w:bookmarkEnd w:id="107" /><w:bookmarkStart w:id="108" w:name="decision-making" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Decision-Making</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">EO technologies - as we have already seen - can be used in a wide variety of geospatial applications. Ultimately however these data need to be used to generate new insights, drive decision-making, and improve our ability to manage and solve complex environmental problems. This aspect of EO is relatively under-developed. In environmental research, EO data is typically used as a data source to investigate questions about spatial distribution or change over time. Conclusions are drawn by analyzing the quantities of interest visually or statistically and/or correlating them with field measurements. However scientists are generally interested in discovering processes, confirming hypotheses, improving their understanding of how processes are operating, rather than specifically developing policies or solutions that address these problems. Academic and government scientists are major users of EO data, and have been key players in improving and developing the science and technology of EO for the past several decades. In Canada for example, federal agencies such as Environment Canada, Natural Resources Canada, and Agriculture and Agri-Food Canada all have extensive EO programs for monitoring aspects of the environemnt from space.</w:t></w:r></w:p><w:bookmarkEnd w:id="108" /><w:bookmarkEnd w:id="109" /><w:bookmarkStart w:id="113" w:name="X5185cbd4f3c0e763a18b48b46600923f562d3c7" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Crop Monitoring in Canada - Earth Observation in Action I</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">One of the most obvious geospatial applications of EO is agriculture. Agriculture is vitally important industry to virtually all countries and is highly dependent on a variety of natural processes such as weather and climate, soils, water resources, as well as socio-political dimensions of environmental regulation. Thus EO has potential applciation in agriculture at a variety of scales from providing information about how crops are growing within individual fields, all the way up to entire agricultural regions, such as The Prairies in western Canada.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Agriculture and Agri-Food Canada (AAFC) is the federal agency overseeing agriculture and the food prodcution industry in Canada. They use EO to produce an annual</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">crop inventory</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">dataset each year that identifies what crop is being grown in each pixel covered by the dataset. This data is produced by combinign several EO datasets, and mapping their spectral characteristics to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">in-situ data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">where the crop types are known. This allows the development of an algorithm to predict crop type from space in an auomated manner, enabling national-scale monitoring of agriculture from space.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 3</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId110"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Take a look at the AAFC Annual Crop Inventory Dataset here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Use the address search tool to zoom into your home town and try to find the nearest farm to where you grew up. If you grew up in an urban area you may have to zoom out a bit; if you are not from Canada or from an area with no agriculture around, just use your current address. Make a note of what seems to be a common crop type for a specific farm.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Next go to the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId111"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">same location in Street View using Google Maps</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Try to use the Street View feature in Google Maps to get a ground view of the farm, or zoom in to get the highest resolution overhead view of the farm. Navigate around and explore the different view on the same location.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Answer the following questions;</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1016" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">What was the dominant crop type as predicted by the crop inventory dataset?</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1016" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">What was different about viewing the data using ACI vs. using Google Maps/Streetview? Which is the more useful way to view the farm? Why?</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1016" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Which view of the farm do you think would be more useful for government? What about the owner/operator of the farm? Why?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3196187" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 11: Clicking on a pixel in the web-map of the AAFC Annual Crop Inventory gives the history of predicted crop types for that location" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/aci.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId112" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3196187" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 11: Clicking on a pixel in the web-map of the AAFC Annual Crop Inventory gives the history of predicted crop types for that location</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BlockText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">End of week 3, start of week 4.</w:t></w:r></w:p><w:bookmarkEnd w:id="113" /><w:bookmarkStart w:id="126" w:name="earth-observation-data-for-digital-earth" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Earth Observation Data for Digital Earth</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">So how does EO relate to DE? Well, EO comprises a large share of DE data. As EO programs continue to grow, the historical archive of image data provides a longer time series with which to compare current trends to. The question that often arises when tracking and monitoring environmental change is;</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">how unusual is this change we are observing today?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Answering this question requires temporal data. For some types of data such as weather stations, this often goes back to the 1950s or 1960s. The longest continuously running EO program is the Landsat program, which dates back to the 1970s. Since we are now on the verge of launchign Landsat 9, with each successive iteration of a Landsat sensor, technology has improved and things change slightly. For example, in Landsat 1-Multi-Spectral-Scanner (MSS), channel 1 is the green band, and the spatial resolution of each pixel is 80 m, while in Landsat 8, channel 1 reflects blue/violet light, and the spatial resolution is 30 m. Thus in order to do longterm DE studies, often some data processing (e.g., to align green bands on imagery from different sensors) is needed to harmonize data so that things line up as expected.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">What we will cover in this week is how DE has changed and is changing traditional EO technologies and tools. As more data is collected, archived, and made accessible - more sophisticated applications and questions can be designed that make use of these data resources. This integration of disparate EO datasets is a key backbone of the DE vision as articulated in Week 1 and echoed in the keynote video at the start of of this lesson.</w:t></w:r></w:p><w:bookmarkStart w:id="117" w:name="hyperspectral-data" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Hyperspectral Data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">One relatively recent development has been the expansion from</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">multi-spectral</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">image data, which records usually 4-15 bands of spectral data, to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">hyperspectral</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">image data, which might have hundreds of image bands over the same chunk of the electromagnetic spectrum. As you would expect, that means these bands are much narrower, and thus sensitive to very small regions of the spectrum. We can visualize the difference in how multispectral bands vs. hyperspectral bands cover the spectrum in the image below.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="585258" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 12: Hyperspectral (Hyperion) and multispectral (ALI) image bands relative to the electromagnetic spectrum, (NASA image by Robert Simmon.)" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/hyperion_bands_diagram.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId114" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="585258" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 12: Hyperspectral (Hyperion) and multispectral (ALI) image bands relative to the electromagnetic spectrum, (NASA image by Robert Simmon.)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The obvious question that arises is why do we need so many bands of data? Hyperspectral imaging essentially covers the the entire spectrum of visible, near IR, and short-wave IR electromagnetic radiation. In this way, a spectra can be built up showing the reflectance curve of the surface being imaged across these wavelengths. This is analogous to spectroscopy studies which can detect chemical constituents of surfaces based on their spectral profiles. As such, hyperpsectral EO can be used to detect actual minerals and chemicals present in the surfaces being mapped. A question that emerges with hyperspectral data in particular, but also with the DE data more generally, is how to organize all of this data so that it is accessible and easy to use. In traditional EO analysis, image data are stored as files on disk, in common formats such a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId115"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">geotiff</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId116"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Mr.Sid</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. However as the data volume gets bigger, managing and accessing data in files becomes challenging. You can imagine if you had a directory of 7 million word documents on your computer, finding the one that had your grade 11 English assignment might be challenging. Your approach to finding might be to sort by name if you knew the file name or used a naming convention, or sort by date to try to narrow down to files that are from the approximate time frame. This is the task of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">information retrieval</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and is a huge area of research in itself (Google is essentially the product of information retrieval research - where the information to be retrieved was relevant web pages to a given text query).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BlockText" /></w:pPr><w:r><w:t xml:space="preserve">So what is the Google for the Digital Earth?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Unfortunately, it does not exist (yet). Different technologies have been proposed for how to organize EO data so that they are quickly accessible, but a leading platform has not emerged. We’ll quickly review one of the more popular approaches.</w:t></w:r></w:p><w:bookmarkEnd w:id="117" /><w:bookmarkStart w:id="120" w:name="data-cubes" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Data Cubes</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">A data cube is a logical extension of the regular square array of pixels we store image data in. In an single image data file is a square, if we stack a bunch of image data files together, we get a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">data cube</w:t></w:r><w:r><w:t xml:space="preserve">. Just like any index, a data cube needs a way to organize its data (similar to how an index in a book is organized alphabetically). A 3-dimensional data cube might have latitude, longitude, and time as the three dimensions of the cube. As such a given pixel defined by the intersection of a latitude and longitude will be aligned with values for that pixel at all other times in the cube. We usually denote these three dimension X Y and Z.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4994776" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 13: Visual depiction of an earth observation data cube, source: Kopp et al. 2019, https://www.mdpi.com/2306-5729/4/3/94" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/cube.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId118" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4994776" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 13: Visual depiction of an earth observation data cube, source: Kopp et al. 2019,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId119"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://www.mdpi.com/2306-5729/4/3/94</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The key property of a data cube is that all of the hard work goes into getting data into the representation but once it is there it is fast and easy to query and access its information. For example, a data cube of landsat data over all of Canada would have to first define what pixels are in and out, how to process raw landsat scenes into just the Canada pixels, how to deal with pixels on edges of scenes, how to deal with aligning image bands from different landsat sensors, how to handle differences in atmospheric, and many other issues. However once all of these problems are solved and the data are indexed in the cube, it is fairly easy to query and use information from massive amounts of data. One of the other very useful properties of the data cube is that new quantities can be computed and stored as layers in the cube. So for example, precomputing vegetation indices for each year and storing those would enable simple analysis of vegetation over time. Since these are only computed once and then stored, they are very fast to access by computer programs, as opposed to computing them</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">on-the-fly</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in response to a user’s request/input.</w:t></w:r></w:p><w:bookmarkEnd w:id="120" /><w:bookmarkStart w:id="125" w:name="Xfeae43ba33591847081d5386044b9fcc519a5e7" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Google Earth Engine and Data-Analytic Pipelines</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Regardless of where and how EO data are stored, we also need analysis tools to generate answers and insights to the problems we’re working on. In the vast majority of studies and applications using EO data, the generic workflow model is as follows</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="1275347" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 14: Analysis workflow for earth observation data" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/workflow.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId121" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="1275347" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 14: Analysis workflow for earth observation data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">While this workflow model has served the EO and geospatial community well for decades, as the DE becomes a reality and EO data archives become larger, this model of working with data is no longer feasible. It requires massive amounts of bandwidth for moving data from the web to your local machine, requires massive amounts of storage space, and massive amounts of computer memory for working with large data files. A more modern workflow is to keep data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">in the cloud</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">– that is, on servers on the Internet, and do processing and analysis via a web browser. This workflow - which we might call a cloud-enabled EO workflow, allows massive amounts of data to be processed and analyzed from virtually any computer with a web browser.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">One platform that has made the cloud-enabled EO workflow a reality in recent years is</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId122"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://earthengine.google.com/</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(GEE). GEE is a completely</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">‘</w:t></w:r><w:r><w:t xml:space="preserve">cloud-based</w:t></w:r><w:r><w:t xml:space="preserve">’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">platform which includes access to massive amounts of DE data, in additio</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Here is an example of the workflow based on GEE in a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId123"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">recent paper mapping the year trees were planted in California orchards</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. In this research project, different types of tree-planting scenarios are detected by the temporal pattern in their NDVI as recorded in Landsat EO data. Do you remember what NDVI measures? Why would this be a good indicator of tree growth?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="1985790" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 15: Google Earth Engine workflow for EO analysis, sample analysis for a project mapping tree age in California orchards, source: Chen et al. 2019" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/gee.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId124" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="1985790" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Think - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">What do you think the benefit of using GEE for this application was? Can you think of any disadvantages to taking this approach?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The cloud-enabled EO workflow and its incarnation in GEE is a huge step forward in realizing the DE vision.</w:t></w:r></w:p><w:bookmarkEnd w:id="125" /><w:bookmarkEnd w:id="126" /><w:bookmarkStart w:id="128" w:name="high-resolution-imaging" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">High Resolution Imaging</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">High-resolution EO can be defined roughly as sensors generating data products at 10 m resolution and under. This means that as resolutions get</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">higher</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">– for example the high resolution optical imager on the Pleiades satellite senses data at 2.8 m, GIS-MS on GeoEye satellite is 1.65 m, and Radarsat2 records in at 1 m – finer objects and land surface features can be detected and monitored. The number of high resolution EO sensors has increased dramatically over the past decade and is enabling new types of geospatial applications.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:hyperlink r:id="rId92"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read through this blog post which compares high and moderate resolution imagery</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, pay attention to the folowing terms: coverage, tasking, cost, spectral bands, and history. Note the source of this article and consider when reading through.</w:t></w:r></w:p><w:bookmarkStart w:id="127" w:name="object-based-image-analysis" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Object-based image analysis</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">One particular feature of high resolution that is unique, is its ability to be used to detect</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">spatial objects</w:t></w:r><w:r><w:t xml:space="preserve">. A spatial object is any discrible object in an image that can be accurately detected and extracted. As noted in our discussion of image classification, being able to add meaning to groups of pixels – as landcover categories, or in the case of high resolution imagery, as individual buildings, ponds, and roads. Once these groups of pixels can be treated as discrete objects, much richer analysis of their distribution, properties, and change-over-time can be realized. High resolution EO data - due to its fine level of spatial detail, requires new ways of doing analysis. For moderate and lower resolution data pixel-based analysis approaches are widley used. However for high resolution analysis, utilizing information about</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">geographic context</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is even more important. Object-based image analysis is a suite of methods that share two broad steps. First is what is called segmentation, and this means grouping pixels together into homogeneous chunks. Just as if you are looking at the roof of a building, it will primarily be one colour, we would automatically group all of the roof pixels together based on their spectral characteristics (e.g., colour). The second step is to classify these segments to give them meaning, so labelling roofs as roofs and so on. At this point different ways of combining labelled segments together can be created, in order to create so called hierarchies of objects, such that a road segment within a park is classified differently than a road segment outside of a park. These sorts of rules can be built up to create very informative and detailed maps derived from high resolution imagery. As you might expect, this sort of analysis is more common in urban landscapes where there is a wide variety of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">object-like</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">features in close proximity.</w:t></w:r></w:p><w:bookmarkEnd w:id="127" /><w:bookmarkEnd w:id="128" /><w:bookmarkStart w:id="130" w:name="X3f3e14fad313eff9bd5481ff307bfe0c5436a9e" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Monitoring Forest Carbon From Space - Earth Observation in Action II</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Forests are critically important for climate change, acting as both carbon sources or carbon sinks. Forests act as sources of carbon when they are burned down or decay after dying. However, when trees photosynthesize they convert carbon from the atomosphere to organic mass making up the tree, and in this way forest growth acts as carbon sink. Thus, monitoring forests is a critical part of national-scale carbon accounting projects, which countries must do to measure progress towards carbon reduction targets. In Canada, there is an estimated 347 million hectares of forested land, making this the a critical information need. Given the huge landbase covered by forests, understanding their role in the carbon cycle is a perfect application for EO approaches.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">One way EO data is utilized to help monitor Canada’s carbon budget is through the National Deforestation Monitoring System. This is a system of mapping and reporting technologies designed to work together to measure how much forest is lost in Canada in a given year. Note that deforestation is not the same as harvesting trees or trees lost to wildfire, rather deforestation is the direct human-induced conversion of forested land to non-forested land. Lansat imagery is the core EO data used for mapping forested areas and detecting deforestation events, in addition to a suite of related geospatial data products including aerial photography, park boundaries, oil &amp; gas industry data, and high resolution imagery. The figure below shows how Landsat data are combined from different time periods to detect forest loss.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2993629" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 16: Examples of various Landsat satellite band combinations and change enhancements that may be used in the mapping process. Note the change enhancement at right shows red triggers where vegetation loss or change has occurred. Band combinations shown are normal colour rendition on left, with colour infrared in the middle. The right-hand column shows Landsat Thematic Mapper (TM) and Enhanced Thematic Mapper (ETM) bands 4,5,3 (i.e., the two near infrared bands and a red band) displayed as red, green, blue, source: Dyk et al. 2015" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/deforest.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId129" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2993629" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 16: Examples of various Landsat satellite band combinations and change enhancements that may be used in the mapping process. Note the change enhancement at right shows red triggers where vegetation loss or change has occurred. Band combinations shown are normal colour rendition on left, with colour infrared in the middle. The right-hand column shows Landsat Thematic Mapper (TM) and Enhanced Thematic Mapper (ETM) bands 4,5,3 (i.e., the two near infrared bands and a red band) displayed as red, green, blue, source: Dyk et al. 2015</w:t></w:r></w:p><w:bookmarkEnd w:id="130" /><w:bookmarkStart w:id="132" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">We have spent these two weeks examining how EO technologies have transformed our abilities to map and monitor natural and human processes at unprecedented scales. We have rich and growing archives of EO data from some satellites, and new and higher resolution EO data coming from newer commmercial satellites. We also have new ways of analyzing these data to extract key insights, and a move towards a cloud-based workflow for people working with EO data. There is clearly a synergy betweeen the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">queryable earth</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">digital earth</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">that is now becoming a reality and fuelling more and more geospatial applications. Being able to understand, critically evaluate, and use these applications are important skills and knowedge for applying the DE to the problems you are most interested in.</w:t></w:r></w:p><w:bookmarkStart w:id="131" w:name="key-terms-from-this-week" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key terms from this week</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">image data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">geostationary orbit</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">sun-synchronous orbit</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">multi-spectral</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">hyper-spectral</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">image classification</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">cloud-enabled EO workflow</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1017" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">spatial object</w:t></w:r></w:p><w:bookmarkEnd w:id="131" /><w:bookmarkEnd w:id="132" /><w:bookmarkEnd w:id="133" /><w:bookmarkStart w:id="165" w:name="Xe84a3c1c20d0855dbd56673ae12b67ceb495baf" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">The View from the Ground: Citizen Science and Community Mapping</w:t></w:r></w:p><w:bookmarkStart w:id="138" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="136" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1018" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId134"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Goodchild, M. F. (2007). Citizens as sensors: The world of volunteered geography. GeoJournal, 69(4), 211–221.</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1018" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId135"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Guardian on Countermapping</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="136" /><w:bookmarkStart w:id="137" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1019" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe two ways individual citizens can contribute to Digital Earth projects</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1019" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Differentiate between volunteered geographic information and citizen science</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1019" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe three limitations of citizen-generated geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1019" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Provide an example of a community mapping project</w:t></w:r></w:p><w:bookmarkEnd w:id="137" /><w:bookmarkEnd w:id="138" /><w:bookmarkStart w:id="140" w:name="this-digital-earth-includes-people" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">This Digital Earth includes People</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Much of the discussion of the Digital Earth we’ve had so far has focused on what we might call</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">top-down technologies</w:t></w:r><w:r><w:t xml:space="preserve">: satellites, sensors, data cubes, online platforms. These have impressive capabilities to create, manage and provide access to geospatial data about the earth. However these tools provide little in terms of agency - that is - little opportunity for</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">you</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to actively participate in their design and operation. This is important because - as we want to emphasize throughout this course - there are design decisions that go into these technologies that can have major impacts on how they are used. In this chapter we want to showcase an alternate set of tools, technologies and trends that have provided for a primary role for citizens in creating and operatlionalizing the digital earth.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">You may have heard the term</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">web 2.0</w:t></w:r><w:r><w:t xml:space="preserve">, or rather - if you were born in the 1990s you may have not heard that term because all you have known is web 2.0. The early internet was in some ways characterized as a one-way form of communication. People could create content and put it up on website, and link to other websites via hyperlinks - which linked web-based documents together. In a way, the early Internet was the wild west as it was fairly simple with a bit of basic coding skills to put up a website; and many of the web-pages and online content were created by amateur-hobbyists. As the Internet matured into the late 2000s, more of the content was authored by corporate entities (e.g., companies, journalists, etc.). As this was occuring, the Internet was transformed by a fundamental change in how people interacted with online content. Instead of simply viewing content - Web 2.0 websites allowed people to actively interact with content: posting comments, liking and disliking posts, adding their own text, video and audio content. If you want to know more about the early Internet -</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId139"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">watch Halt and Catch Fire</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(especially season 4).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">A similar evolution can be traced in the DE. While the DE is very much in essence a top-down one-way type of technology analagous to the early Internet, a suite of geospatial technologies have evolved that have enabled ordinary citizens to create their own geospatial data. The biggest factor in enabling this transformation was the widespread adoption of mobile phones equipped with location sensors (e.g., GPS chips). Coupled with Web 2.0, these two technologies (i.e., mobile phone GPS chips and websites supporting user-generated content) created a unique opportunity for citizen and community-generated geospatial authoring. What we saw as a result has been described by various terms: Volunteered Geographic Information (VGI - Goodchild et al 2007.), user-generated geographic content (UGG - X), or geographic citizen science (Haklay 2010). Whatever the term, the result has been an explosion in the production of geospatial data. These types of data can also be an important component of the DE. For example we saw in the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId67"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">marinetraffic.com</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">example how user-genrated content in the form of pictures and videos of ships and ports added rich geographical context information to the ship location data provided by the AIS sensors.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">This sort of information fusion is a key characteristic of the DE.</w:t></w:r></w:p><w:bookmarkEnd w:id="140" /><w:bookmarkStart w:id="158" w:name="X6ce863627648f5ee62d9892fc166b19321d051f" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Citizen Science, VGI, and CBM: Empowering people through mapping</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In 2007, Geographer Michael Goodchild gave voice to this emerging trend of citizen-empowered mapping in</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId134"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">an article titled</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">“</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Citizens as sensors: The world of volunteered geography</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">”</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. This highly inlfuential paper begins with a recounting of the naming of America (the continent) by a little-known European mapmaker in the year 1507. The story illustrates that throughout history geographic information has been produced through a variety of ways; mapmakers, explorers, colonialists, victors in wars, etc. In fact, place names are often the outcome of highly politicized processes. In many parts of Canada for example, modern place names of natural features are names given by European settlers and colonial powers (e.g., United Kingdom, France and Spain), ingnoring their existing names used by Indigenous peoples already residing there. As such there is a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId141"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">movement in many parts of Canada to re-adopt Indigenous place names</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Authority to designate official geographic place names has become the sole purview of governmental bodies at various levels. As Goodchild notes in his article, attempting to change a place name today is extremely difficult. And there are often compelling reasons for wanting to change geographic place names; for example in our own backyard there has been a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId142"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">longstanding effort to attempt to rename a controversial stretch of road in Puslinch</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId143"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">(note this debate is ongoing today)</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Read through and watch these resources to get familiar with the debate in Puslinch.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Debates over naming and renaming geographic features are often highly contentious. Why do you think this is?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">List two arguments for and against changing the name of the road in Puslinch.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">What side of the debate would you be on? Can you think of any other process we could use to name geographic features?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Unlike in the early days of map-mapmaking, geographic data can now be created in many different ways. The DE concept is founded on a holistic digital representation of the places we work, live, and play in on a daily basis. The Goodchild paper also recounts how three core geographic</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">enabling technologies</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">have contributed to the explosive growth of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">volunteered geographic information</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Georeferencing</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in the broadest sense is the process of attaching geographic coordinates to non-geographic information. Such non-geographic information may in fact have its own internal coordinate system, but must be anchored to the earth through the process of georeferencing. Think about if you take a photo with your phone and download it to your computer, it will often have geographic coordinates attached the image (it might show up on a map when viewing it as well). This is a georeferencing process of the photo using the phones internal GPS sensor. The coordinates are latitude and longitude - the most common geographic positioning system used for locating positions on the earth. Geographic coordinates are great because they work for any location on earth and they are easily stored and processed by computers. However, if I asked you to tell me your thoughts on 43.476160, -80.524969 you might have some difficulty. You would need to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">map</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">these coordinates and see their relation to other geographic context information to make sense of the question.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId144"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">You can do this easily by searching for coordinates in Google for example</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Humans cannot naturally work with geographic information in the form of coordinates very well. As such - alternate ways of recording positional information have been devised such as street addresses. For instance, 75 University Avenue West is much more memorable than 43.473559, -80.527479. However even addresses, although much more memorable, have problems. For example 5 King Street Ontario could be located in any number of towns; we’d need province and postal code to be sure of the location. Other locales have less developed street addressing systems (e.g, small villages without paved roads) - how can we locate positions in these areas? The</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId145"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">what3words</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">project linked to above attempts to solve this problem by cutting up the entire world into 3 m x 3 m square cells, and attaching three random words to each cell. Thus</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId146"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">amends.explores.test</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">vs. </w:t></w:r><w:hyperlink r:id="rId147"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">consequences.gaping.quack</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">vs. </w:t></w:r><w:hyperlink r:id="rId148"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">enchanted.clashed.revving</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">all refer to separate distinct locations. It turns out that by simply using three word strings, there are enough unique combinations to index every 3 m x 3 m square cell on the entire planet (i.e., over 57 trillion cells). One of the issues with that system is there is no logical ordering so you have no way to know about the relative positioning of location references like you do using geographic coordinates.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Whether we use geographic coordinates, or street address, or alternative systems like what3words, when we attaching these geographic references to other information we are in way georeferencing that information. In practice, we almost always want to eventually translate back to geographic coordinates. The particular case of translating street addresses to geographic coordinates is called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">geocoding</w:t></w:r><w:r><w:t xml:space="preserve">. The process of georeferencing has been made much easier and more extensive by two primary technologies. Firstly, GPS, which is a type of global navigation satellite system (we’ll discuss in detail next week) - has moved from being available only in standalone receivers at degraded positional accuracy, to availble on tiny chips with meter-level accuracy. This has led to location-based apps and services, that are so common today. This also means that people can contribute geospatial data via apps and websites relatively easily. Secondly, webmapping systems have been developed which allow people ot associated new information with geographic coordinates by clicking on an online map which records the click and translates that from screen coordinates to geographic coordinates. This process is sometimes called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">heads up digitizing</w:t></w:r><w:r><w:t xml:space="preserve">. With these two tools for creating geospatial data now so widely available, it is no wonder that VGI and user-generated geographical data have greatly increased the overall prodcution of geospatial data.</w:t></w:r></w:p><w:bookmarkStart w:id="157" w:name="openstreetmaps-osm" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">OpenStreetMaps (OSM)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The OpenStreetMaps (OSM) project is a web mapping application that aims to create a map of the world through</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">crowdsourcing</w:t></w:r><w:r><w:t xml:space="preserve">. You can compare Figure 4 in the Goodchild article to the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId149"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /><w:i /></w:rPr><w:t xml:space="preserve">state of the map</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">today</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. You will notice that there is a lot more detail in the current version compared to the version of the map in the 2007 article, attributed to more and more people contributing to mapping the streets in their community. Today OSM map data is used in a wide variety of applications. In fact, many web mapplications now use OSM data as their</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">basemap data</w:t></w:r><w:r><w:t xml:space="preserve">. The idea of an open source collaborative process to create geospatial data has both benefits and challenges. On the one hand, there often less control organization to the mapping effort, as people are volunteering their time and contributing mapping edits when it is convenient for them. This can result in</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId150"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">popular areas of the map having lots of detail</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">,</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4425911" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 17: Screenshot of OSM public GPS trace data near Stanley Park, Vancouver, BC" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/osm.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId151" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4425911" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 17: Screenshot of OSM public GPS trace data near Stanley Park, Vancouver, BC</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">while more</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId152"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">remote areas or less frequently visited areas</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">are missing and/or incomplete. The first location is the entance to Stanley Park in Vancouver, British Columbia, while the second location is in the city of Rathnapura in the central region of Sri Lanka. These maps show public GPS traces uploaded to OSM in these two different locations. Seeing the Sri Lanka city we see GPS traces present basically on the major roadways perhaps in line with public transportation routes. Whereas on the Vancouver map we see many many overlapping traces on roadways, sidewalks, trails into the park, and so on. Why do you think this would be? This variation in</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">coverage</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of OSM data is an important issue if the data is to be used for things like routing. One of the more challenging aspects of variable coverage in OSM data is that the variation tends to correlate with things that are related to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">who</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is doing the mapping - often people with some sophistication with technology, money, more so in Europe and North America than are regions, etc. These sorts of systematic biases in the data can greatly reduce its overall quality and usefulness.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId153"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">One study that compared OSM data to official government data in the United Kingdom</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">(Haklay 2010) found that in the 10 wealthiest neighbourhoods 76% of total road length was mapped in OSM, whereas in the 10 poorest neighbourhoods only 46% of the total road was mapped (this study was done in 2010).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Why do you think there may be systematic biases in OSM data and/or in VGI more generally? Can you think of an example where this sort of bias could be really important? How could bias in who participates in a project like OSM be reduced?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">There are countless examples of how such</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">crowdsourced</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">data can fill in gaps in official data sources or provide insights into problems or issues where no information exists. At Laurier for example there is a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId154"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">project called RinkWatch</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is a crowdsourcing project where people report when they can skate on outdoor/backyard rinks during winter. Tracking theses observations over time, we can see how people’s outdoor skating activities - a storied part of Canadian culture - is sensititve to changes in weather and climate. Such data about people’s skating activites simply does not exist in any other form, so eliciting public engagement and input on the RinkWatch project has</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId155"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">been key to exploring this issue</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="1994789" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 18: Screenshot of the RinkWatch homepage" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/rw.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId156" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="1994789" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 18: Screenshot of the RinkWatch homepage</w:t></w:r></w:p><w:bookmarkEnd w:id="157" /><w:bookmarkEnd w:id="158" /><w:bookmarkStart w:id="160" w:name="community-mapping" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Community Mapping</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">VGI projects like OSM desribed above tend to be focused on individual participation; recruiting interested individuals to contribute their time and energy to a project of interest. There are also more collective mapping exercises which are generally described as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">community mapping</w:t></w:r><w:r><w:t xml:space="preserve">. In a community mapping project, people get together to develop a project around a common issue; usually an environmnental issue that requires geospatial data collection, collation and sometimes analysis. The notion of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">counter mapping</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">has been used to describe community mapping projects where communities map a theme or issue of interest in direct oppposition to corporate or state powers. The counter mapping process is collaborative, iterative and often non-digital; where people mark up paper maps to designate areas and highlight concerns. Countermapping thus has the potential to give voice to typically marginalized communities and to promote environmental justice through authoring of geospatial data and stories. From the perspective of the DE, these sorts of rich community data can provide critical local context to abstractions of geospatial data. A</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId135"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">short article in the Guardian provides a brief overview of some countermapping</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">projects.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">A great example of community mapping can be found on</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId159"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">greenmap</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which includes community mapping projects from around the world. The greenmap project provides a consistent methodology and software for communities to deploy in mapping local areas - either for generic mapping or around a community concern. There are hundreds of examples of greenmaps in cities around the world, initiated by community groups, city governments, university groups, and others. Have a look through some of the greenmap projects - can you think of an issue you would could create a greenmap for in your community? What would it be? What type of information would you want to map?</w:t></w:r></w:p><w:bookmarkEnd w:id="160" /><w:bookmarkStart w:id="162" w:name="augmented-reality" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Augmented Reality</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In the examples we have talked about so far, we have primarily been describing ways that citizens and communities can get involved in</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">creating</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">geospatial data, either to participate in a wider project initiated by others (e.g., scientists, government) or through a collaborative community process. However citizens can also utilize the DE to their own ends by bring the digital world into the physical world of their lived experience - this means bringing down location-based digital data to one’s perception of the world around them. This may sound a little odd but is surpsingly common. Location-based services on your smartphone use your phone’s GPS chip to send geographic information to an app that uses that information to send you</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">geographically relevant</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">information. This information might then shape how you interact with the world around you.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">A recent example of location-based services is contact tracing apps - made popular in the wake of efforts to contain COVID-19 around the world. The principle behind some of these apps is that location data are recorded on the app, and when someone tests positive for COVID-19, their location history can be compared against location histories of other users to determine potential exposures. In Canada, the contract tracing app provided by the federal government is voluntary, so may suffer from some of the issues of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">coverage</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">noted in our discussion of VGI. In the case of contact-tracing apps as a location-based service, the location element only ever becomes relevant in retrospect (assuming the postive case goes into quarantine after learning of their test result). The</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">historical</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">location history then is activated and compare against other peoples historical location history (and/or location check-ins at distinct locations). In the Canadian app - called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId161"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">COVID Alert</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">- the app was actually engineered to not technically use GPS data. Instead, the app actually broadcasts and receives random numbers to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">nearby</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">phones using Bluetooth, then for any positive test the codes of exposed people are known and they can be notified. This system has the benefit of protecting peoples</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">geoprivacy</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and is not dependent on access to GPS satellites (i.e., Bluetooth will work indoors). The geospatial component in COVID-Alert is therefore based on the range of the Bluetooth signal (~10 m) which is a good proxy for potential exposure to COVID-19. Bluetooth radio waves however do not depend on line-of-sight - can you think of an example of why this might be an issue?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Location-based services have been extended to more visual applications which allow you to visualize geospatial data within the actual geographic context you are located in. This is best described by simply seeing it, which you can see in this promo video for a local augmented reality / geospatal company:</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 3</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">How could AR such as that in the video above be used in another context such as emergency response or policing? How does visualizing geospatial data in-the-field change how you might perceive or understand the data?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkEnd w:id="162" /><w:bookmarkStart w:id="164" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In the preceding chapters we talked about the vision for the DE, how its been applied, and how the growth of satellite-based earth observation is fuelling new ways to monitor natural and social processes at global and local scales. The primary elements in this discussion were technologies. In this chapter we sought to highlight how people are involved in the DE, primarily through the production of geographic information through VGI, citizen science, crowdsourcing, and community mapping. These tools give communities new ways to help shape their worlds and solve environmental problems in their own communities. Viewed from the perspective of the DE, these tools can be leverage the vast sources of geospatial data to enrich their own observations and data collection efforts. The advent of augmented reality is a new technology enabling the fusion of remote geospatial data from the DE with the local geographic context being experienced by the user. As a greater synthesis of remote and localized DE tools becomes a reality, there will be new ways for individuals and communities to respond to and anticipate change.</w:t></w:r></w:p><w:bookmarkStart w:id="163" w:name="key-terms" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key Terms</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">volunteered geographic information</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">coverage</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">crowdsourcing</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">community mapping</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">georeferencing</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">geocoding</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1020" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">top-down vs. bottom up mapping</w:t></w:r></w:p><w:bookmarkEnd w:id="163" /><w:bookmarkEnd w:id="164" /><w:bookmarkEnd w:id="165" /><w:bookmarkStart w:id="189" w:name="gps-and-wayfinding" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">GPS and Wayfinding</w:t></w:r></w:p><w:bookmarkStart w:id="171" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="169" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1021" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">[Brief history of GPS technology]</w:t></w:r><w:hyperlink r:id="rId166"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://news.stanford.edu/pr/95/950613Arc5183.html</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">)</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1021" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId167"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Wayfinding Article</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1021" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId168"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">GPS and the Human Journey, video: 28:31</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="169" /><w:bookmarkStart w:id="170" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1022" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explain how GPS works</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1022" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Give three examples of how GPS is used today</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1022" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Define three types of wayfinding tasks</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1022" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe how GPS has changed our wayfinding abilities and behaviours</w:t></w:r></w:p><w:bookmarkEnd w:id="170" /><w:bookmarkEnd w:id="171" /><w:bookmarkStart w:id="181" w:name="what-is-gps" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">What is GPS?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">You have no doubt heard the three letters GPS before, and perhaps even know what it stands for - global positioning system. For a technology that is largey associated with space: locating positions on the earth’s surface, navigation, tracking, etc. it is a little ironic that GPS is actually</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">all about time</w:t></w:r><w:r><w:t xml:space="preserve">. One of the early inventors of GPS was Roger Easton who worked for the US Naval Research Laboratory. In early 1960s Roger was experimenting with long-range</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId172"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">bi-static radar systems</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and needed a way to synchronize the clocks for the two radar stations. Any two clocks will eventually drift out of sync - and resolving the discrepancy between the two clocks is a problem known as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId173"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">clock synchronization</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Roger ended up using a highly accurate clock called an atomic clock (aka cesium beam clock) to sychronize the two clocks by driving the 100 mile distance between the two stations. This was a rather inefficient method of resolving the time syncrhonization issue for the radar system, and eventually led to the idea that an atomic clock on a satellite could broadcast time corrections to the two radar stations simulataneously, synchronizing the clocks and saving on gas in the process. Satellite-based clock synchronization was the fundamental idea underlying the development of GPS technology. With such a clock onboard a satellite, the sending and receiving time of the signal could be determined, and given that the signal is travelling at a constant speed (theoretically) -</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">the speed of light</w:t></w:r><w:r><w:t xml:space="preserve">, the time could be converted to a distance. Let’s take a closer look at this idea.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">We start with a ground-based receiver receiving a signal broadcast from a satellite. The satellite is continuously broadcasting its atomic clock time. This time is convereted to a distance - given that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>d</m:t></m:r><m:r><m:t>i</m:t></m:r><m:r><m:t>s</m:t></m:r><m:r><m:t>t</m:t></m:r><m:r><m:t>a</m:t></m:r><m:r><m:t>n</m:t></m:r><m:r><m:t>c</m:t></m:r><m:r><m:t>e</m:t></m:r><m:r><m:t>=</m:t></m:r><m:r><m:t>s</m:t></m:r><m:r><m:t>p</m:t></m:r><m:r><m:t>e</m:t></m:r><m:r><m:t>e</m:t></m:r><m:r><m:t>d</m:t></m:r><m:r><m:t>×</m:t></m:r><m:r><m:t>t</m:t></m:r><m:r><m:t>i</m:t></m:r><m:r><m:t>m</m:t></m:r><m:r><m:t>e</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and we know that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">light travels at around 300,000 km/s</w:t></w:r><w:r><w:t xml:space="preserve">, if a signal sent by the satellite might take 0.07 seconds to arrive at the receiver, we could easily see that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>300</m:t></m:r><m:r><m:t>,</m:t></m:r><m:r><m:t>000</m:t></m:r><m:r><m:t>k</m:t></m:r><m:r><m:t>m</m:t></m:r><m:r><m:t>/</m:t></m:r><m:r><m:t>s</m:t></m:r><m:r><m:t>×</m:t></m:r><m:r><m:t>.07</m:t></m:r><m:r><m:t>s</m:t></m:r><m:r><m:t>=</m:t></m:r><m:r><m:t>21</m:t></m:r><m:r><m:t>,</m:t></m:r><m:r><m:t>000</m:t></m:r><m:r><m:t>k</m:t></m:r><m:r><m:t>m</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">, which is about where a GPS satellite sits in orbit when directly overhead.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId174"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Try plugging these values into a calculator and see how they work out</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Now if that satellite is further along its orbital path in the horizon, that time value will be larger, and the distance will increase accordingly.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4845309" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 19: GPS satellite trilateration Image source: GeoCommons" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/2.141.gif" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId175" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4845309" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 19: GPS satellite trilateration Image source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId176"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">GeoCommons</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">What we know at that point is that the receiver is 21,000 km from the satellite. We can envisions this as being on the surface of a sphere where the satellite is at the centre, and the sperical radius is 21,000 km. Since we could be located anywhere on this hypothetical sphere it’s not enough to determine our location. Imagine we connect to another satellite, and determine our distance from that satellite is 27,000 km. At this point we know we are located on the surface of two theoretical spheres, and in fact two such spheres will intersect along a circle, so we have to be located somewhere on the circle where the two spheres intersect.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">If we add a third known distance my connecting to yet another satellite, we can now be located at only two possible location, where all three spheres intersect. If we have an approximate idea of where we are (e.g., if one is on earth and one isn’t), we can likely discard one as incorrect, or if we add more satellites we can then determine our location exactly. This is how GPS works, via a constellation of satellites broadcasting time information which is then received by GPS receivers in your phone or on your handheld GPS device to determine your exact location. The GPS system that was launched by the US as the first global navigation satellite system called NAVSTAR GPS was comprised of a global constellation of 24 satellites.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="3048000" cy="2438400" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 20: GPS constellation Image source: WikiCommons" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/ConstellationGPS.gif" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId177" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="3048000" cy="2438400" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 20: GPS constellation Image source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId178"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">WikiCommons</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In this figure the blue point represents a GPS receiver. As the earth rotates on its axis and the GPS satellites move along their orbits, the receiver can connect to a different number of sensors. Note as above, we only need 4 satellites to detect our location on earth, but due to a variety of sources of error, we typically want more satellites to determine an accurate location.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="2450592" cy="3694176" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 21: Artist’s view of the deployed TIMATION-1 satellite (Original image credit: NRL) Image source: https://directory.eoportal.org/" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/image_gallery.jpeg" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId179" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="2450592" cy="3694176" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 21: Artist’s view of the deployed TIMATION-1 satellite (Original image credit: NRL) Image source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId180"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://directory.eoportal.org/</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Returning to the early development of the GPS program by the US, Roger Easton’s satellite-based clock synchronization scheme was tested in a program funded by NRL called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId180"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">TIMATION</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, which launched and operated two small satellites 1967 and 1969 to demonstrate the concept of using synchronized clocks that provided passive ranging signals.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId166"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">TIMTATION was one of three competing satellite based programs (TIMTATION, 621B, and TRANSIT)</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which in 1973 were integrated into one program for satellite-based navigation called NAVSTAR GPS. NAVSTAR GPS launched several satellites into orbit over the next decades while refining the developing the space-borne satellite technology and the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">control segment</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">responsible for signal processing, sending commands and data to satellites, overall maintenance and monitoring of satellites. In 1994 the 24th GPS satellite was added to the system providing truly global coverage for navigation and positioning. The development of GPS as a military technology had clear applications in navigation, identifying targets, and many other uses for naval, air force, and army operations. However the civilian applications of GPS which we depend on today were more of an afterthought. Once GPS was up and running as military technology, civilian use was eventually enabled, however the positioning accuracy was intentionally degraded for non-military uses through a process called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">selective availability</w:t></w:r><w:r><w:t xml:space="preserve">. Thankfully, selective availability was disabled by the US Government in 2000, ushering in an era of high-accuracy satellite-based navigation and positioning for a wide array of civilian and commercial uses.</w:t></w:r></w:p><w:bookmarkEnd w:id="181" /><w:bookmarkStart w:id="185" w:name="wayfinding" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Wayfinding</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">GPS data make up a significant component of human-oriented Digital Earth data. Much of the VGI and location-based services discussed last week depend on accurate and ubiquitous positioning information. GPS technology has become so ubiqitous we expect to be able to find our way wherever we are on earth. If you are dropped down into a random location anywhere on earth, you would probably expect to open your phone, see your position, and see some geographical context data that would help you to figure out where you were and where to go. Let’s try it;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId182"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Go to this site that generates random geographic coordinates</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and click Pick Coordinates. Note; since the earth is</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId183"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">70% covered by water</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">you may have to click a few times to find on a spot on land. On the first land spot you find, zoom in as much as you can to see the area. Expand the view to full screen (upper right corner) so you can get a feel for the area. Then slowly zoom out and make a plan for how you would get home if dropped in that location. Assuming you had this map and your location pinned on the map, how would you find your way to get home?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Now, zoom in or out until the scale bar at the bottom of the map says 5 km. Then click Pick Coordinates until you find another land spot. Once you do, try to figure out where you are. Look for clues, what information are you looking for to help you understand your geographic context? Landscape type, place names, are there geographic borders or street names? If you can’t see anything, zoom out one level. Once you are fairly certain of where you are, make a note of what the scale bar says (e.g., 5 km, 10 km, 100 km, etc.)</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="914400" cy="914400" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 22: Distracted wayfinders in a common urban scene. Image source:“We stand so close together, but we are so far apart” by Ed Yourdon is licensed with CC BY-NC-SA 2.0." title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/phone.jpg" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId184" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="914400" cy="914400" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 22: Distracted wayfinders in a common urban scene. Image source:</w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">We stand so close together, but we are so far apart</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">by Ed Yourdon is licensed with CC BY-NC-SA 2.0.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The process of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">finding your way</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is part of a field of study known as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">wayfinding</w:t></w:r><w:r><w:t xml:space="preserve">. Wayfinding can be defined as goal-directed movemennt to a distant location that cannot be directly perceived by the traveller. Thus, wayfinding is an extremely common human activity. You may ask why is this a topic worthy of scientific study? It turns out that there are extensive individual differences in wayfinding abilities. On average, men tend to do better than women, and younger people tend to do better then older people. Disorientation and inability to navigate is also associated with Dementia symptoms and people with Alzheimer’s disease. But these skills are essential to our understanding and experience of the world. Why then does wayfinding apply to our understandign of the DE? There are two primary reasons. Firstly, the widespread adoption of GPS and assisted navigation aids have led to declines in traditional wayfinding and spatial cognition skills. Secondly, understanding the nature of human wayfinding in natural environments may shed light on how people navigate in digital environments. Much research has explored wayfinding in digital worlds, and as we develop new immersive DE applications, wayfinding through digital worlds constructed of geospatial data may soon be a reality.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Wayfinding can classified into three types of tasks.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1023" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Goal-directed movement to reach a familiar destination</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1023" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Exploratory movement with the goal of moving back to a known location</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1023" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Goal-directed movement to reach a novel destination</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">You can think of examples of these that you engage in every day. Commuting to school or work is movement towards somewhere you already know (task 1) is a different wayfinding activity from trying to find your hotel from the airport wheh you travel to a new city (task 3). Once you get to the hotel and get comfortable, you might head out to explore your surroundings, with the intention of ultimately coming back to your hotel (task 2).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Take 30 minutes and watch this video with the author of a recent book investigating wayfinding.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Describe a time when you arrived in a new location and had to find your way. How did you do it?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkEnd w:id="185" /><w:bookmarkStart w:id="186" w:name="ethical-issues-of-gps-tracking" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Ethical Issues of GPS Tracking</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In a world where so much of our lives are tracked and encoded as digital transactions, increasingly our geographic location is either explicitly or implicily revealed through our use of apps and location-based services. The notion of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">geoprivacy</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">has been introduced in the academic literature to encapture the need to preserve peoples right to their location data and information about their movements. As data are filtered and processed through more complex systems of apps, services, and related data processing tools many of which are owned and exist for commercial profit, the control of one’s own location data becomes difficult. GPS tracking data have enormous value to companies to reveal human movement patterns to help them target advertising, to governments hoping to inform planning and management of public places, as well as to law enforcement agencies when seeking to identify the whereabouts of a person at a given time. The use of GPS trackign data for these purposes is only possible when people either explicitly consent to the use of their location data by third parties, or as is more common, people have no agency to control how their data is used after is collected. Are you concerned about preserving your</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">geoprivacy</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">when you use location-based apps and services? Why or why not?</w:t></w:r></w:p><w:bookmarkEnd w:id="186" /><w:bookmarkStart w:id="188" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The evolution of GPS from its early days in the 1970s to the current state of the art of has transformed how we create geospatial data - but also how we move and interact with geographic space. As we have seen, wayfinding and navigation is a critical human skill with links to memory, identity, and ultimately what it means to be human. As more of our navigation and wayfinding tasks are coupled to GPS-enabled devices, there may be important information we are missing about coming to know our surroundings. The links between the technological revolution of GPS and the changes in how we navigate are still being discovered.</w:t></w:r></w:p><w:bookmarkStart w:id="187" w:name="key-terms-from-this-week" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key terms from this week</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1024" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">gps</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1024" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">clock synchronization</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1024" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">gnss</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1024" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">selective availability</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1024" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">wayfinding</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1024" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">geoprivacy</w:t></w:r></w:p><w:bookmarkEnd w:id="187" /><w:bookmarkEnd w:id="188" /><w:bookmarkEnd w:id="189" /><w:bookmarkStart w:id="219" w:name="X6cacdcfde6936d57c1bc490378f271946db55ad" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">Unmanned Aerial Systems for Earth Observations</w:t></w:r></w:p><w:bookmarkStart w:id="195" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="193" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1025" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId190"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Remote Damage Assessment after Tornado Hazard by using UAS</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1025" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId191"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Fly your drone safely and legally (Government of Canada)</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1025" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId192"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Where can you fly your drone?</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="193" /><w:bookmarkStart w:id="194" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1026" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe characteristics of differnt types of UAS in terms of mapping</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1026" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">List common sensors available for UAS-based mapping</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1026" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Find examples of geospatial applications or UAS-generated data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1026" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Overview key regulations that pertain to operating UAS in Canada</w:t></w:r></w:p><w:bookmarkEnd w:id="194" /><w:bookmarkEnd w:id="195" /><w:bookmarkStart w:id="202" w:name="uas-for-mapping" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">UAS for Mapping</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Drones! When used for environmental research and monitoring, a drone is actually one component in what is commonly called an</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">unmanned aerial system</w:t></w:r><w:r><w:t xml:space="preserve">. The components of a UAS include:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1027" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">the Unmanned Aerial Vehicle (UAV) - aka drone</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1027" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Radio/GPS</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1027" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Flight control</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1027" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">A pilot</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1027" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Controller</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1027" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Flight programming software</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2456447" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 23: Components of an unmanned aerial system, image credit: Branden Walker" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/uas.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId196" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2456447" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 23: Components of an unmanned aerial system, image credit: Branden Walker</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In order to understand the role that UAS play in the field of environmental monitoring and research, we can compare them to competing platforms for collecting geospatial imagery. As we have seen in Week 2, satellite based earth observation is an increasingly popular method of acquiring earth information. There are several important differences between satellite based EO and UAS-based mapping:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1028" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">satellites are in fixed orbits and data can only be acquired during a pass over the target site, UAS can be deployed anytime local conditions permit</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1028" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">UAS data tends to be much higher in spatial resolution; acquiring images where each pixel is centimeters not several or hundreds of meters as is the case with satellite imaging</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1028" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">for quickly moving phenomena that require rapid mapping (e.g., wildfire spread), satellite EO may not have fine enough temporal resolution (i.e., revisit times) whereas UAS can be deployed at any desired temporal resolution</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1028" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">in general,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">most</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">UAS have less spectral bands than what is commonly available from EO sensors. However advanced sensors can be mounted on UAS including thermal, lidar, and hyperspectral sensors.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1028" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">there are greater operational considerations in flying UAS for data collection than in obtaining EO data from a satellite. In EO data, all of the planning and complexity goes into putting the satellite in orbit, so once it is there it stays somewhat predictable. With UAS, ground-based considerations can be a big factor in where you can and where you can’t fly a UAV. Meticulous planning is required to carry out a UAS mission for image acquisition.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">So UAS can play a critical role in the collection of geospatial information when information needs require high spatial resolution imagery over a relatively small region, and ground conditions dictate that flying a UAV is legal, safe, and will generate useful geospatial data.</w:t></w:r></w:p><w:bookmarkStart w:id="198" w:name="platforms" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Platforms</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">A key distinction between mapping UAVs and generic drones (even though these categories can and do overlap), is that those used for mapping must have automated flight capability. That is, a flight plan is set in advance and the UAV carries out the flight plan via autonomous flight. Usually flights are guided by GPS on board the UAV itself as well as capability to be radio controlled with a remote control. There are two main platforms designs used in UAS mapping.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Multi-rotar</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">UAVs are vehicles with several top-mounted propellers which can maneurver into difficult spots and can be engaged to follow targets, avoid objects, and hover in the same position. The rotor engines move in alternating directions and a series of sensors automatically adjusts for effects of wind and momementum to keep the platform stable. Flying times for multi-copters vary widely, but usually range from 20 mins to 1 hr. As such these platforms are typically used for mapping small areas within line of sight of the operators. Rotary-UAVs can take off and land vertically, enabling them to be launched from small areas / complex terrain (e.g., in forest canopy).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2101904" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 24: Fixed wing and multi-rotor platforms for UAV mapping, image credit: Branden Walker" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/platforms.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId197" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2101904" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 24: Fixed wing and multi-rotor platforms for UAV mapping, image credit: Branden Walker</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Fixed-wing</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">platforms have a central body and usually rear or front-mounted engines and/or electric-powered propellers that drive the vehicle forward, generating lift under their wings. Air speed and direction sensors and ailerons control/steer the UAV.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Fixed-wind UAVs cannot hover, but have a larger range, can cover large areas in a single flight (~100 ha), and generally offer more stable conditions for image acquisition. For these reasons many mapping UAVs have been designed as fixed-wing UAVs. Take off and landing of fixed-wing UAVs require clear and open areas.</w:t></w:r></w:p><w:bookmarkEnd w:id="198" /><w:bookmarkStart w:id="201" w:name="sensors" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Sensors</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Just like a camera can have different lenses, a UAV platform can be outfitted with different sensors. Sensors differ in terms of what types of light they are sensitive and whether they are a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">passive</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">active</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">sensor. Passive sensors record reflected light and are basically cameras that image the ground. While UAV-based cameras can be controlled and take images at a variety of angles, for mapping we usually want a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">vertical</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">orientation for the photo, so that distortions near the edges of the photo are minimized. Passive sensors can be sensitve to visible light just like a normal camera, or as a multi-spectral sensor where Red/Green/Blue light is sensed separately as distinct image channels. With multi-spectral data the image channels can then be manipulated and enhanced in different ways. An near-infrared channel is often part of multi-spectral sensors, which are able to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">see</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">into part of the electromagnetic spectrum that that we can’t see with the human eye. This is especially important for monitoring vegetation health, detecting stress in plants and crops before they might be visible with the human eye. For this reason,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId199"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">UAVs have become increasingly used in agriculture</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Another type of sensor common in UAV mapping is a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">thermal sensor</w:t></w:r><w:r><w:t xml:space="preserve">.If you have ever</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId200"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">seen any of the Predator movies</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">you have seen thermal imagery. A thermal sensor as you might expect sense temperature differecens of the scene it is imaging. Thermal sensors are typically much more expensive than standard visible-range sensors. A thermal sensor on an UAV can be used for different applications. For example, because water content is correlated with temperature, thermal variation can detect drought stress in plants or leaks in pipelines. One key advantage of UAV mapping is that different sensors can be added to a UAV platform for a specific project, greatly expanding the types of environmental projects a single UAV can contribute to.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Having reviewed some of the potential uses of UAVs in agriculture.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">write down three potential issues</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">that might arise in deploying UAVs in this context</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The second type of sensor is</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">active sensors</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which transmit some energy and record what is reflected back by the surface. The type of energy transmitted differs depending on the type of active sensing, but the most common for UAV mapping is called light detection and ranging (LiDaR). LiDaR data sends electromagnetic energy pulses as waves in the near infared part of the spectrum (.90</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>μ</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">m and 1.5</w:t></w:r><m:oMath><m:r><m:t>μ</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">m - which are micrometers or units of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:sSup><m:e><m:r><m:t>10</m:t></m:r></m:e><m:sup><m:r><m:t>−</m:t></m:r><m:r><m:t>6</m:t></m:r></m:sup></m:sSup></m:oMath><w:r><w:t xml:space="preserve">m in length).</w:t></w:r></w:p><w:bookmarkEnd w:id="201" /><w:bookmarkEnd w:id="202" /><w:bookmarkStart w:id="210" w:name="d-mapping-from-uas" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">3D Mapping from UAS</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">One of the fundamental differences between traditional</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">mapping</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and the digital earth, is in the treatment of the third dimension, elevation. In the early days of computer mapping and GIS, research and development efforts primarily focused on automating the process of mapping using computers. To this end, the objective was still to make a two-dimensional map that could be plotted on a piece of paper, rather than to use the digital represetntaiton itself as a new way to model the earth and geographic processes. While much progress has been made, elements of traditional 2D mapping remain rooted in many GIS and mapping applications. However, 3-dimensional representations are increasing and new data models and methods are being developed to generate, manage, analyze and visualize geogrpahic information in full 3-dimensional detail. UAVs have been on the forefront of these changes through the method of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">structure-from-motion</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">photogrammetry. Before we explore this exciting new technology, we can briefly review what traditional photogrammetry is.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The photogrammetry is what it sounds like: making measurements from photographs. In the context of mapping this means measuring both distances and heights from aerial photographs. Photogrammetry has been traditionally employed with aerial photography from aircraft. A key property of photogrammetric work is to exploit the property of parallax which is the apparent shift in the position of an object when viewed from different perspectives. Classic photogrammetry used overlapping photographs and a viewing device called a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">stereoscope</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to see</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">in stereo</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and view portions of the sensed area in 3D. Thus measurements can be made between the base and top of an object for example, to measure its height. Here is an example of a classic stereoscope set-up for airphoto intrepretation</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">While photogrammetry is useful for making measurements of 3-dimensional objects in aerial photography, the visualization and exploration of 3D environmental data was limited. We will see how UAVs have drastically changed 3D measurements</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">visualization of geospatial data at very high resolution.</w:t></w:r></w:p><w:bookmarkStart w:id="204" w:name="structure-from-motion-photogrammetry" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Structure from Motion Photogrammetry</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Classic photogrammetry using steroscope viewers or even more advanced set ups using computer-vision devices are primarily a manual process of interacting with geospatial data to link and measure aspects of the scene in the photographs. A new type of photogrammetry has emerged in the last decade which is almost entirely automated.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Structure-from-motion</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">photogrammetry works according to the same principles as classic photogrammetry; obtaining overlapping images along the flightline, identifying the same objects in adjacent aerial images, calculating their geometric properties through triangulation of the object in multiple scenes. The key innovation here is that instead of having to match objects manually; thousands of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">tie points</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">are generated which map the same locations in multiple images together. Because the triangulation of objects in the scene is three-dimensional, we can use SFM to generate 3D representations of the landscape called a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">point cloud</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="7058986" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 25: Adjacent aerial images obtained from a UAV in northern Canada, image credit: Branden Walker" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/dual-snow.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId203" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="7058986" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 25: Adjacent aerial images obtained from a UAV in northern Canada, image credit: Branden Walker</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">We see in the image above two adjacent images from a UAV mapping campaign in northern Canada. The success of an SFM analysis of images is to successfully identify objects in multiple scenes so their position can be triangulated. This requires substantial variability to be present in the scene so the same points in multiple images can be linked. Here things like the bases of trees, tops of trees, shadows, differences in snow textures etc.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Can you think of any environments or landscapes where SFM might not work? Why would that be?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Perhaps the most important part of SFM is what we can generate from the triangulated image objects in the scene. Because we have 3-dimensional coordinates, we can generate 3-dimensional imagery outputs which allow us to explore three dimensional structure of landscapes. Have a look at this</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">3D point cloud</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">obtained from UAV flight campaign and SFM processing of the data;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">SFM can also generate digital surface models (DSM) where the height of the surface is computed from the point cloud as well as a digital elevation model (DEM) where the ground elevation is computed. These three derived products; point clouds, DSMs, and DEMs can be used in a wide array of geospatial applications and surve as important basemap data for the study site.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 3</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">What added benefit is there to viewing the landscape in 3D as opposed to a standard 2D map?</w:t></w:r></w:p><w:bookmarkEnd w:id="204" /><w:bookmarkStart w:id="209" w:name="Xe2b376d60c1396513a6ab7b2dc653c2533912aa" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Case Study: Drones for mapping snow in northern Canada</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Several researchers at Laurier incorporate UAS into their research programmes for environmental data collection and mapping. One group of researchers, led by</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId205"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Professor Phil Marsh</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">- have pioneered a snow mapping method using fixed-wing UAVs. Working primarily north of the treeline at</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId206"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Trail Valley Creek Arctic Research Station</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. One of the group’s research objectives is to monitor snowpack and how it changes over time. Thus utilizing structure-from-motion photogrammetry the team use UAVs to generate a digital surface map of a study site during the summer months as a baseline elevation datum. During winter when there is snowcover, they fly another UAV route to generate at winter DSM. Now for each pixel we have two values, the height above mean sea level</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">without snow</w:t></w:r><w:r><w:t xml:space="preserve">, and the height above mean sea level</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">with snow</w:t></w:r><w:r><w:t xml:space="preserve">, and therefore substracting one from the other gives you the snow depth (make sure you understand this).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In this way, the team can fly multiple UAV mapping campaigns through the winter to monitor the snowpack over time. This has important implications for understanding the hydrological regime of the area and how it is responding to changes in climate.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2571657" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 26: Mapping snow depth using structure-from-motion photogrammetry and unmanned aerial vechicles, image credit: Branden Walker" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/snow.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId207" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2571657" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 26: Mapping snow depth using structure-from-motion photogrammetry and unmanned aerial vechicles, image credit: Branden Walker</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">And here we can see the SFM-derived changes in snow water equivelent during the melt period in the landscape. Visualizing how snow differentially melts in relation to landcover and vegetation classes helps to extrapolate water balance changes across the wider landscape where land cover classes can be mapped but are too large for detailed UAV mapping.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2584271" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 27: Visualization of change in snow water equivalent during melt of snowpack in Trail Valley Creek, NT., image credit: Branden Walker" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/swe-melt.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId208" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2584271" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 27: Visualization of change in snow water equivalent during melt of snowpack in Trail Valley Creek, NT., image credit: Branden Walker</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">This is how UAV-based mapping can be incorporated into a broader DE system, generating high resolution data over specific sites which can then be linked to both field level measurements (</w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">in-situ data</w:t></w:r><w:r><w:t xml:space="preserve">) and more large scale earth observation data available from satellites. These three scales of geospatial data acquisition form a hierarchy that can be tailored to address specific environmental information needs and create the most accurate maps, models, and forecasting systems within an integrated DE system. Integration of geospatial data across spatial scales is a core research area for realizing the DE vision.</w:t></w:r></w:p><w:bookmarkEnd w:id="209" /><w:bookmarkEnd w:id="210" /><w:bookmarkStart w:id="213" w:name="uav-mapping-project-planning" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">UAV Mapping Project Planning</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">While recreational drones are relatively common now with many amateurs using them for photography and just for fun, using a UAV for generating useful geospatial data requires a lot of equipment, preparations, and planning.</w:t></w:r></w:p><w:bookmarkStart w:id="212" w:name="uav-flight-planning" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">UAV Flight Planning</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Flight planning requires several considerations about the study site, conditions, the actual flight plan, and logisitical factors. Firstly, the study site should be clear of obstructions, traffic, buildings, etc. Except in rare circumstances where special permits are required, UAV mapping should not be attempted over large groups of people and/or populated areas. Depending on the platform, the take-off/landing area needs to be identified and ensured it is suitable. Any overhead obstructions (e.g., power lines) need to be identified and take account of. The flight altitude/ground resolution and the degree of lateral and medial overlap needs to be specified to ensure enough tie points for either 2D or 3D mapping. Finally the flight time of the planned mission needs to be considered; so that the battery life of the UAV is sufficient to complete the flight path. Flight planning software can be used to simplify these steps and ensure a successful mission.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2989806" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 28: Flight planning software showing flight lines, take off and landing positions for mapping mission, image credit: Alex MacLean" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/flight-plan.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId211" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2989806" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 28: Flight planning software showing flight lines, take off and landing positions for mapping mission, image credit: Alex MacLean</w:t></w:r></w:p><w:bookmarkEnd w:id="212" /><w:bookmarkEnd w:id="213" /><w:bookmarkStart w:id="216" w:name="X336210502b55ed5420864c22b809edbf78f1aee" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Regulations for using UAV for mapping in Canada</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Flying drones is heavily regulated in Canada. As of 2019 you now need a Drone Pilot Certificate to operate a drone that weighs 250 grams or more.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId191"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Read through the regulations provided by the government of Canada</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. One of the most important regulations is to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">NOT</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">fly within 5.6 km of an aerodrome. It might be surprising but that regulation significantly limits where you can fly in populated areas. Have a look at a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId214"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">webmap that captures exclusion areas and areas of caution</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4049167" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 29: National Research Council of Canada Drone Mapping Tool, image source: https://nrc.canada.ca/en/drone-tool/" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/drone-tool.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId215" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4049167" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 29: National Research Council of Canada Drone Mapping Tool, image source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId214"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://nrc.canada.ca/en/drone-tool/</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 4</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">If you wanted to map your current residence would you be within an eligible zone? What would be some limitations you might incur if planning a UAV-based mapping mission over your current residence? What is the difference between orange filled areas and red filled areas on the map?</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkEnd w:id="216" /><w:bookmarkStart w:id="217" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In the last decade the use of drones and UAS for geospatial mapping has increased significantly. As a data source platform for the DE, UAV-based mapping can capture local scale landscapes at very high spatial resolution with a variety of sensors and platform configurations. This extreme flexibilty coupled with the fact that deployment of mapping missions can be tailored to respond to unexpected events on an as-needed basis makes this mode of geospatial data capture very appealing. For these reasons we can only expect the trend of UAV-based mapping to increase.</w:t></w:r></w:p><w:bookmarkEnd w:id="217" /><w:bookmarkStart w:id="218" w:name="keywords" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Keywords</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1029" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">UAS, UAV, drone</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1029" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">VLOS flight</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1029" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">structure from motion photogrammetry</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1029" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">point cloud</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1029" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">tie points</w:t></w:r></w:p><w:bookmarkEnd w:id="218" /><w:bookmarkEnd w:id="219" /><w:bookmarkStart w:id="257" w:name="gis-and-big-data" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">GIS and Big Data</w:t></w:r></w:p><w:bookmarkStart w:id="225" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="223" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1030" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId220"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">From crime mapping to crime forecasting: The Evolution of Place-Based Policing</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1030" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId221"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">How data-driven policing threatens human freedom</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1030" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId222"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">How the police use facial recognition, and where it falls short</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="223" /><w:bookmarkStart w:id="224" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1031" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Appreciate the types of decision making that goes into a big data analysis project with geospatial data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1031" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Ask three critical questions of any data intensive project or study</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1031" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explain what the five v’s of big data are</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1031" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Give an example of how GIS and Big Data can help to both exacerbate and address complex social issues</w:t></w:r></w:p><w:bookmarkEnd w:id="224" /><w:bookmarkEnd w:id="225" /><w:bookmarkStart w:id="227" w:name="overview-of-lesson" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Overview of Lesson</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This week we will take a different approach to learning about the digital earth. Instead of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">reading</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">about it, we will engage in a live analysis of data. This complete week’s lesson will be generated using</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId226"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">R</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, a free and open source, widely used statistical programming language used in data science and modelling.</w:t></w:r></w:p><w:bookmarkEnd w:id="227" /><w:bookmarkStart w:id="236" w:name="X0ee6fa1bbe85148ab15a678e397549082830e18" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Live Worked Example of Geospatial Data Analysis</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(geojsonio)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(sp)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">spdf &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">geojson_read</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;https://opendata.arcgis.com/datasets/af500b5abb7240399853b35a2362d0c0_0.geojson&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">,  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">what =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;sp&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This loaded two packages with the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">command, then the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">geojson_read</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">function went out to a website maintained by the Toronto Police Services through a platform called ArcGIS and retrieved a live file of police crime data. This dataset provides crime statistics for the City of Toronto aggregated by neighbourhood.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId228"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">You can read more about this datset here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. What the code above did was go to that website and retrieve this dataset as a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId229"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">geojson file</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is a text-based file format for interchange of geospatial data. We don’t need to learn all the ins and outs of the format now, but it’s important to note it includes both geometry (in this case neighbourhood boundaries) and tabular dimensions of the data (various crime statistics reported at the neighbourhood level).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Now that we have this data in hand - what can we do with it? For starters we can plot it;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">plot</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdf)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4267200" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="bookdown-demo_files/figure-docx/unnamed-chunk-43-1.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId230" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4267200" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">What is shown here is geospatial data (i.e., data that knows its about its location on the earth). The data are the boundaries of neighbourhoods in the City of Toronto. What is hidden is all of the crime data that is also included in the file we downloaded from the Toronto Police Service. We can get a list of what is included in the tabular data attached to this file</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">names</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdf)[</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">:</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">15</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">]</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">##  [1] &quot;OBJECTID&quot;          &quot;Neighbourhood&quot;     &quot;Hood_ID&quot;          </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">##  [4] &quot;Population&quot;        &quot;Assault_2014&quot;      &quot;Assault_2015&quot;     </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">##  [7] &quot;Assault_2016&quot;      &quot;Assault_2017&quot;      &quot;Assault_2018&quot;     </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">## [10] &quot;Assault_2019&quot;      &quot;Assault_AVG&quot;       &quot;Assault_CHG&quot;      </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">## [13] &quot;Assault_Rate_2019&quot; &quot;AutoTheft_2014&quot;    &quot;AutoTheft_2015&quot;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Now when we view the column names of a dataset, sometimes we can infer what the columns mean, other times it’s more cryptic. It is generally good practice to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId228"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">consult the</w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="Hyperlink" /><w:b /></w:rPr><w:t xml:space="preserve">metadata</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">that is,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">data about the data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which describes the data. If we return to the link above, we can read up on what each of these columns actually stores. Looking here we we can see that crimes are broken down into type, and sometimes linked to a specific year between 2014 and 2019, or given as the average over that period.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Let’s map Break and Enters in 2014 just to get a feel for the data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">spplot</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdf, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;BreakandEnter_2014&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4267200" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="bookdown-demo_files/figure-docx/unnamed-chunk-45-1.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId231" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4267200" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">This allows us to see the downtown core and one of the northwest neighbourhoods had the highest</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">rates</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of break and enter crimes. We can also see general</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">spatial patterns</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">by visualizing the crime data in this way. Before going further we should figure out exactly what we are looking at. The values in the legend range from 0 to 160. Go back to the web page and review the description of the data.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">We would have read that the crime is reported as crime rates per 100,000 people by neighbourhood based on 2016 Census Population. Viewing the data as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">rates</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">makes it posssible compare areas independent of their populaiton (i.e., otherwise areas with highest population would likely have the hights raw totals of crime events). Most crime analysis use crime rates rather than raw counts for this purpose.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The above however is a static map, we can make it more interactive using a web map. One of the most widely used web mapping programs out there is called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId232"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">leaflet</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is a JavsScript library and can do some pretty amazing things. You have already interacted with leaflet maps in earlier weeks. Let’s make a leaflet map as follows;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#load the leaflet library</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(leaflet)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(sf)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">leaflet</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">st_as_sf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdf)) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addTiles</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">() </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addPolygons</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">() </w:t></w:r></w:p><w:bookmarkStart w:id="233" w:name="htmlwidget-cbc6f7f5711c02a66406" /><w:bookmarkEnd w:id="233" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Now this is is an</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">interactive map</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">meaning we can zoom in and out and click on things unlike the static image map we saw about. This interactivity allows for far more exploration of the data. This is especially important when working with</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">big data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">because we may not know in advance everything we want to find out about.</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Exploratory data analysis</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is a type of analysis that emphasize exploration - and there are many tools available for</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">exploratory spatial data analysis</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 1</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Spend some time zooming in and panning around this map. We can see that the default colouring of the City of Toronto neighbourhood boundaries is a transparent blue with a darker blue boundary. Also take a look at the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">basemap</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">underneath - you can see in the status bar below that leaflet is using an OSM basemap - a project we learned about a few weeks ago.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Now we will want to do a little more with our leaflet map because it currently is not showing any crime data.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#convert from an sp object to an sf object</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">spdfsf &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">st_as_sf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdf)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#set the colour scale</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">qpal &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">colorNumeric</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">palette =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;YlGnBu&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">domain =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> spdfsf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">BreakandEnter_AVG)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#call the leaflet function</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">leaflet</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdfsf) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addTiles</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">() </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addPolygons</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">stroke =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OtherTok" /></w:rPr><w:t xml:space="preserve">FALSE</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">smoothFactor =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="FloatTok" /></w:rPr><w:t xml:space="preserve">0.2</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">fillOpacity =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">color =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">~</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">qpal</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(BreakandEnter_AVG))  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addLegend</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;bottomright&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">pal =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">   qpal, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">values =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">~</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">BreakandEnter_AVG,</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">    </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">title =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;Break and Enters Average Count &lt;br&gt; 2014-2019&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">,</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">    </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">opacity =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">  )</w:t></w:r></w:p><w:bookmarkStart w:id="234" w:name="htmlwidget-049316eaabe9833205e7" /><w:bookmarkEnd w:id="234" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Now we have a much better map showing the average crime for Break and Enters for the 2014-2019 period by neighbourhood in the City of Toronto.Nowe can see the spatial patterns in the crime rate a little more clearly. If you take a Cartography class (e.g., GG 251) you will learn all about how selecting the colour scale and the map elements are critical for making an effective map.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The last thing we can do is make this crime map a little bit more interactive. Right now we can see approximately what the crime is by looking at the colour on the map and the legend. But if we wanted to know for example, what is the exact average crime count in Mimico - it would be hard to figure out with this map. We can add more interactivity using the attribute data attached to the dataset to enable this.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">We want to add a few things to this interactive map;</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1032" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">ability to click on a neighbourhood and see its name</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">+ this would help us answer the question</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">what neighbourhood is this</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">when we see something of interest. An alternate way of doing this would be to label every neighbourhood</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1032" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">link to other information about the neighbourhood</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">leaflet</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdfsf) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addTiles</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">() </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addPolygons</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">stroke =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OtherTok" /></w:rPr><w:t xml:space="preserve">FALSE</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">smoothFactor =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="FloatTok" /></w:rPr><w:t xml:space="preserve">0.2</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">fillOpacity =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">color =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">~</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">qpal</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(BreakandEnter_AVG),</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">popup =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">paste0</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;b&gt;Neighbourhood: &lt;/b&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , spdfsf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">Neighbourhood</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;br&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;b&gt;Crime Count: &lt;/b&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , spdfsf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">BreakandEnter_AVG</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;br&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;b&gt;Population: &lt;/b&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , spdfsf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">Population</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;br&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">                 , </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;&lt;a href=&#39;https://www.toronto.ca/ext/sdfa/Neighbourhood%20Profiles/pdf/2016/pdf1/cpa&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">as.numeric</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(spdfsf</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">Hood_ID), </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;.pdf&#39;&gt;View Neighbourhood Profile&lt;/a&gt;&quot;</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">               ))  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">addLegend</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;bottomright&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">pal =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">   qpal, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">values =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">~</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">BreakandEnter_AVG,</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">    </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">title =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;Break and Enters per 100,000 &lt;br&gt; 2014-2019&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">,</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">    </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">opacity =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">  )</w:t></w:r></w:p><w:bookmarkStart w:id="235" w:name="htmlwidget-decbfb3982863f9ee4e1" /><w:bookmarkEnd w:id="235" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">What we see above is a lot of code that is formatting the popup label on the map. You definitely do not need to try to understand all of this. The key thing is we can create a relatively complex interactive map using publicly available data and linking to external report data in a few minutes. One of things we are doing here is using the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">Hood_ID</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">column which is a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">unique identifier</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of each neighbourhood which is also used on the City of Toronto website in the file name of the reports that are PDF format. So we build a link to the external document using the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">Hood_ID</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in the dataset. Connecting data from different sources is a key aspect of working with big data. Often this involves reading</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">metadata</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to find out what specific columns mean and how they link to other information sources.</w:t></w:r></w:p><w:bookmarkEnd w:id="236" /><w:bookmarkStart w:id="243" w:name="geospatial-data-and-crime" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Geospatial Data and Crime</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Now that we have a little bit of a feel for the crime data published for the City of Toronto, we can think more deeply about what it means, how it can be used, and potential sources of bias and/or ethical issues of using these data sources. Crime mapping goes back at least two decades, and is premised on the belief that spatial patterns of crime can be used to understand root causes of crime, to aid in solving existing crimes, and to prevent the occurence of future crime through geographically targetted community policing. Many police departments now employ crime analysts (often geographers) whose job it is to analyze crime data. The existing of crime mapping databases and software has led to the proliferation expansion of the notion of a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">crime hotspot</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is a geographical area with an elevated</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">crime rate</w:t></w:r><w:r><w:t xml:space="preserve">. Seeing as in the mapping above we used the average</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">count</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of crimes but normally we would want to know the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">crime rate</w:t></w:r><w:r><w:t xml:space="preserve">. Crime events are typically standardized by some measure of the size of the population. As such, 5 murders in a neighbourhood composed of 1000 people has a wholly different level of risk than 5 murders in a neighbourhood made up of 10,000 people. Standizing counts of crime events relative to the size of the population helps to understand the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">relative risk</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of crime. Note that crime rates are typically enumerated over some geographic unit. In the example of break and enters in Toronto we used above, the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">geographical unit of analysis</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">was City of Toronto official neighbourhoods. Let’s examine these in a little more detail by zooming in to one particular neighbourhood:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r></w:p><w:bookmarkStart w:id="237" w:name="htmlwidget-ca8b9513a755845f9be2" /><w:bookmarkEnd w:id="237" /><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Here we have made two changes to are previous map. We set the fillOpacity parameter to .6 so that we can now see through the colours a little bit to see the basemap. Secondly, we have zoomed in to a particular part of the east side of the city, along Queen Street East. What do you notice here? If you click north of Queen Street East you find a crime rate of 601.7 in Regent Park, whereas if you click south of Queen Street East in Moss Park, you find a crime rate of 1141.1. If this map is to be believed, the second we cross Queen Street east the crime rate instantly doubles. Is this real? What is happening here? Well to some extent this is really the result of the geographical units over which we computed the crime rate. A key property of these sort of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">areal measures</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is that they really only apply at the entire neighbourhood level, not at any particular location</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">within</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">that neighbourhood. This may seem counterintuitive - because it is - and people frequently misinterpret what these sorts of maps and numbers actually mean.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Now let’s think a little further about what we are seeing here. These are crime rates for crimes which are categorized as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">break and enter</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">crimes. We would normally consult the metadata to find out more about what this crime data actually means - however in reality</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId238"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">the metadata is often lacking as is the case here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. We sometimes have to do some digging to understand the data we are looking at,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId239"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">for example we could look at the definition here</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to see we are talking about property crime. Thus if the make up the housing stock in these two neighbourhoods is different that might explain some of the discrepancy in crime rates.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Stop and Do - 2</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Have a detailed read of the Neighbourhood Profiles for both Regent Park and Moss Park. Try to look for clues that might explain the discrepancies in crime rate between these two neighbourhoods. Write down two sentences about how .</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">you think police might use this data to do their job. Can you think of any dangers or risks of using these data to inform police practice?</w:t></w:r></w:p><w:bookmarkStart w:id="242" w:name="what-is-crime-data" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">What is Crime Data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">If you have seen the movie above -</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId240"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Minority Report</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">- you probably know this was a science fiction movie about crime prevention that released in 2002. The basis of the plot is that an advanced technology allowed police to see when crimes were going to happen</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">before they occurred</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">so that the future criminal could be apprehended and the crime prevented. While far-fetched futurism in 2002, today police services around the world are using big data and geospatial tools to attempt to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">forecast</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">crime and deploy resources in advance, not too far behind what was depicted in the movie.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Crime data such as those we explored above are really a measure of criminal activity, one that has sources of bias and significant limitations. If we imagine crime as an invisible process happening out in the world which we are trying to learn about, we can envision police statistics of criminal offenses as one limited view into this process. It is worth thinking about all of the sources of error, bias, and noise that impact this view. Things such as</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1033" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">type of crime - So-called white-collar crime (e.g., fraud) might be less likely to be detected than street crime</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1033" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">shifts - when policing shifts start and end and number officers deployed at a given time impact the overall policing effort and likelihood of detecting crime happening in the community</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1033" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">location bias - if police effort is not deployed evenly, but in relation to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">perceived</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">crime risk - such as in so-called hot-spot policing, crime is more likely detected in these area and less likely to be detected in other areas</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1033" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">racial profiling/discrimination - if police disproportionately stop or pre-emptively investigage members of a certain race or ethnic group, they</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">may</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">show up disproportionately in crime statistics</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId241"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">an issue which has been in the news in recent years in Toronto</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">And these are just a handful. It is important to realize that crime data are not a true reflection of criminal activitiy, but rife with sources of bias and error in addition to complexities of working with aggregte data expressed as rates.</w:t></w:r></w:p><w:bookmarkEnd w:id="242" /><w:bookmarkEnd w:id="243" /><w:bookmarkStart w:id="249" w:name="crime-data-as-a-case-of-big-data" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Crime Data as a case of Big Data</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Interest in a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">data-intensive</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">approach to policing which requires extensive data, statistical modelling, geospatial forecasting, and related tools and technologies has increased greatly in recent years. This transformation in policing is analagous to how</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">big data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">coupled with advanced machine learning and artificial intelligence algorithms is transforming almost every industry and field. What does it mean for data to qualify as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">big data</w:t></w:r><w:r><w:t xml:space="preserve">? Usually big data is defined according to the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">multiple V’s</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">model of big data, which relates to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1034" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Volume - the actual size of data - as measured in quantities of bytes (which itself is a collection 8 bits, where each bit which can store either a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>1</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>0</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">)</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="1912733" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 30: Growth in amount of global data over time" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/big-data.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId244" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="1912733" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1034" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Velocity - measures the speed at which data is created or accumulated. Data may stream in in 1 byte per hour or 2 mb per milisecond.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1034" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Variety - the format and type of data has increased greatly in recent years. Even with purely geospatial data, there are more formats and varieties of geospatial data (e.g., vector formats, raster formats, video formats, sound formats, etc.)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">these are the core</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">V’s</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">of big data. But two others have been defined more recently as well, these are:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1035" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Veracity - which is about the quality of data, which can be highly variable and lead to biased or incorrect results</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1035" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Value - is the insight and value derived from analyzing larger datasets. This is an often overlooked aspect of big data but is crucial. If we need a bunch of new computational platforms, algorithms, and skilled professionals all to analyze data and learn nothing more than what we already knew in the first place - is it really worth it? Value of big data is derived from</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">new</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">insights made possible by seeing patterns and connections which are otherwise invisible.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="5061966" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 31: The five V’s of big data, source: https://medium.com/@suryagutta/the-5-vs-of-big-data-2758bfcc51d" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/big-data2.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId245" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="5061966" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 31: The five V’s of big data, source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId246"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">https://medium.com/@suryagutta/the-5-vs-of-big-data-2758bfcc51d</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Each of the five dimensions of big data apply to the case study of crime data as well, increasing in volume and velocity as more GPS sensors are deployed, web-based reporting replaces paper-based methods, and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId247"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">overall more parts of police practice are digitized</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and coupled to analytics. More variety in data - from point events associated with street addresses of service calls to bodycam video footage, more variety of formats is now created. Data quality issues are harded to track and less frequently reported but are likely increasing as well as the volume of data increases. A recent example of data quality issues in a military setting provides a clear example of the worst case scenario, where</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId248"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">positional errors of up to 13 metres were known and ignored in operations used for drone strikes in Iraq</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Being a critical and informed consumer of geospatial data and all big data services requires thinking carefully about how data are created, processed, and utilized.</w:t></w:r></w:p><w:bookmarkEnd w:id="249" /><w:bookmarkStart w:id="254" w:name="crime-statistics-in-city-of-toronto" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Crime Statistics in City of Toronto</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">In our leaflet maps we made above, we explored only one variable in the dataset,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">BreakandEnter_Rate_2019</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which reports the 2019 rate of break and enter crimes at the neighbourhood level. What if we had a more generic question about crime in Toronto, such as</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1036" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">where is crime increasing the most?</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1036" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">what types of crime are increasing and what types of crime are decreasing?</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">These two questions are much more interesting than mapping a single crime rate as we did above, because these questions ask about</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">change</w:t></w:r><w:r><w:t xml:space="preserve">. Often we know the general patterns of high and low are, but what is most useful is to know how things are</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">changing over time</w:t></w:r><w:r><w:t xml:space="preserve">. This can then feed into where resources should be allocated, where social programs are needed or are working (or not), and so on. When seeking to answer a question by consulting data it is helpful to break it down into discrete, well-defined chunks. Often the process of breaking the question down requires making some explicit assumptions.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Let’s look at the first question</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">where is crime increasing the most?</w:t></w:r><w:r><w:t xml:space="preserve">. First of all, the question is asking about</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">where</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">so we know the answer will have to be some sort of geography. Given the limitations of the dataset we are working with, the answer will be one or more neighbourhoods. Secondly, we need to define what</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">crime</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">means; are we talking all crime? We might need to seek further clarification here to answer this. We will focus on answering this question for non-violent crimes; Break and Enters, Robbery, Theft Over $5000. And of course, we are limited by the window into crime which is offered by the analysis of police statistics data, which as we noted above is imperfect. Next we need to define what is meant by</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">increasing</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and how to measure it. One way would be to compare the difference between the 2014 and 2019 rate. Another way would be to try to fit a trend line to all years of data for each neighbourhood. Since we only have one population value -taken during the 2016 census, we can compare the counts directly. We will look at the difference in crime for the types above between 2014 and 2019 and then map the results to try to answer the question about where crime is changing.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">When we put together a recipe to answer a question with data, we are building an</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">algorithm</w:t></w:r><w:r><w:t xml:space="preserve">. In its simplest form, and algorithm is simply a set of instructions which are explicitly laid out so that a computer can follow them and return a result. Our algorithm consists of the following steps:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1037" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">sum crime counts for non-violent crimes for each neighbourhood for 2014 and 2019</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1037" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">subtract the 2019 count from the 2014 count</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1037" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">divide the output of step 2 by the 2014 count</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1037" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">map the output of step 3 using an interactive map</w:t></w:r></w:p><w:bookmarkStart w:id="250" w:name="htmlwidget-5779d6688bc5e527738d" /><w:bookmarkEnd w:id="250" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Examine the map above? What would your answer be to the question we were trying to answer?</w:t></w:r></w:p><w:bookmarkStart w:id="252" w:name="exercise" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Exercise</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The January 2017 report from the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId251"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Toronto Police Service titled; Action Plan: The Way Forward Modernizing Community Safety in Toronto</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">includes the following quote;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BlockText" /></w:pPr><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">More effective use of data, GPS and other geo-spatial enabled technologies should be a major priority.They enable better solutions that can also be implemented quickly. Also, a principled approach to more transparent data and information sharing is a key component of building public trust.</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This illustrates how the Digital Earth and Big Data are converging in policing. There are both advantages and disadvantages to this approach.</w:t></w:r></w:p><w:bookmarkEnd w:id="252" /><w:bookmarkStart w:id="253" w:name="notes-question" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Notes Question</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Come up with three questions that you could pose to the Toronto Police Service about this strategy.</w:t></w:r></w:p><w:bookmarkEnd w:id="253" /><w:bookmarkEnd w:id="254" /><w:bookmarkStart w:id="256" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">This week we have taken a more applied approach to understanding our topic; big data and GIS. We worked through an actual live analysis of a crime dataset obtained from the Toronto Police Service. We also read about how</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">big data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">is transforming policing, and the dangers of relying on biased data as a sole source of truth. Given the widespread social issues surrounding policing, racial stereotyping, and the surveillance society we now live in, understanding big data issues through the lens of crime illustrates how current and important these issues are. However, issues of biased data, opaque algorithms, and privacy breeches underly all types of big data analysis and many dimensions of the Digital Earth. Becoming a critical consumer of the Digital Earth requires a basic literacy in these issues.</w:t></w:r></w:p><w:bookmarkStart w:id="255" w:name="key-terms-from-this-week" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key terms from this week</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">metadata</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">geographical unit of analysis</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">data quality</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">bias</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">algorithm</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">big data</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1038" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">crime rate</w:t></w:r></w:p><w:bookmarkEnd w:id="255" /><w:bookmarkEnd w:id="256" /><w:bookmarkEnd w:id="257" /><w:bookmarkStart w:id="300" w:name="geovisualization-and-story-maps" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">GeoVisualization and Story Maps</w:t></w:r></w:p><w:bookmarkStart w:id="262" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="259" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1039" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId258"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">University Consortium for Geographic Information Science - Geovisualization Chapter</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1039" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">reading 2</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1039" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">reading 3</w:t></w:r></w:p><w:bookmarkEnd w:id="259" /><w:bookmarkStart w:id="261" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1040" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Use geovisualization tools to explore geospatial earth system data available online</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1040" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Describe four different dimensions of viewing the digital earth</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1040" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Explain how a story map differs from a more standard web map</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1040" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Discuss limitations of visualization</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1040" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Provide two examples of what the future of digital earth geovisualization might look like</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The term</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">geovisualization</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">can be defined as</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">&gt;</w:t></w:r><w:r><w:t xml:space="preserve">“</w:t></w:r><w:r><w:t xml:space="preserve">process of interactively visualizing geographic information in any of the steps in spatial analyses, even though it can also refer to the visual output (e.g., plots, maps, combinations of these), or the associated techniques.</w:t></w:r><w:r><w:t xml:space="preserve">”</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">As data comprising the Digital Earth continues to grow (in volume and variety as discussed last week), new tools and approaches are needed to</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">visualize</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">these data. Visualize can mean maps, graphs, or other visual forms that show a graphical representation of the data. The field of effective visualization is an important part of the analytic toolkit, and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId260"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">great guides exist</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which can help you learn about</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">how</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to visualize data. Data is often used for decision making: to inform policy development, to decide on a company strategy, as part of an environmental review of impact assessment. Even the most meticulously collected data can be ineffective at influencing decision-making if it is improperly visualized. Even when very complex statistical alorithms are employed with huge datasets to generate forecasts or model predictions, the results of these models are usually visualized in some way to communicate their results.</w:t></w:r></w:p><w:bookmarkEnd w:id="261" /><w:bookmarkEnd w:id="262" /><w:bookmarkStart w:id="281" w:name="viewing-the-digital-earth" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Viewing the Digital Earth</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth differs fundamentally from other types of big data in that information referenced to the earth (i.e., geospatial data) and therefore have a natural way to view them (i.e., maps!). Mapping - whether in traditional or interactive/online forms provide a tool for exploring spatial variation in a thematic dimension of the Digital Earth. Maps are by definition two-dimensional views onto the digital earth. We will use the idea of data dimension as a way to explore the world of visualization tools available for Digital Earth data.</w:t></w:r></w:p><w:bookmarkStart w:id="268" w:name="one-dimensional-views" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">One-Dimensional Views</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Any graphical form which we view features of the data along one dimension of space/time, we usually describe as 1-dimensional data. The most common form of 1-D data visualization is a time series plot, where time is measured on the x-axis and another variable is measured on the y-axis. You are likely very familiar with interpreting these sorts of graphs</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;climate&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;tidyverse&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#df = meteo_noaa_hourly(station = &quot;010080-99999&quot;, year = 2019)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">df &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">read.csv</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;SV.csv&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">df2 &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">df </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">group_by</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">day=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">as.Date</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">format</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">as.Date</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">str_sub</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(df</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">date,</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">,</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">10</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">), </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;%Y-%m-%d&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">), </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;%D&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">), </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;%m/%d/%y&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">%&gt;%</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">summarize</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">maxTemp =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">max</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(t2m, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">na.rm =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OtherTok" /></w:rPr><w:t xml:space="preserve">TRUE</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">))</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">plot</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(df2</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">day, df2</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">maxTemp, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">type=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;l&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">xlab=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;Date&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">ylab =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;Temperature (Celsius)&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4267200" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="bookdown-demo_files/figure-docx/unnamed-chunk-54-1.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId263" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4267200" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">This is just a simple static plot, but we probably would like an interactive version, which we can do easily.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;ggplot2&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">library</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;plotly&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:br /></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">p &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">ggplot</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(df2, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">aes</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">x=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">day, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">y=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">maxTemp)) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">+</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">     </w:t></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve"># set plot axes</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">geom_point</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">shape=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">1</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">+</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">                       </w:t></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve"># Use hollow circles</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">geom_smooth</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">() </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">+</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">                             </w:t></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve"># add smoother   </w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">  </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">labs</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">x =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;Day&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">y =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&quot;Temperature (Celsius)&quot;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve"># add labels</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">ggplotly</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(p) </w:t></w:r></w:p><w:bookmarkStart w:id="264" w:name="htmlwidget-30b3bd24148333b4127c" /><w:bookmarkEnd w:id="264" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Figure 32: Maximum temperature recorded in 2019 in Svarbald, Norway, data source: US National Oceanic and Atmospheric Association</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Here we have used an interactive plot. You can use the buttoms in the top bar of the graph to pan and zoom in to different parts of the plot. You can also mouse over individual data points to see their actual values. If you click on the blue line, this gives the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">model value</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">provided by the smoothing curve. We can think of this as predicted value of maximum temperature for a corresponding date. We might use this to estimate max temperature in a future (i.e., unknown) year. However - if this year was particularly cold or warm, it might not be a good estimate. This is why most weather forecasting takes into consideration long-term averages called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId265"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">climate normals</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">In this view of the data, we also decided not to use a line to connect each observation, but instead used a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">smoothing function</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to visualize the general structure of the data. Does this help you see the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">temporal pattern</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">in the max temperature data? It is not suprising perhaps that the warmest temperatures are in mid-summer. Give the location of this station,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId266"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">located here in Svalbard, Norway</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">, even the warmest temperatures are still fairly cool days. With can also use the location</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId267"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">to have a look at what the landscape looks like</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">Explore this 360 degree photo and see if you can see the meteorlogical station where this data we are plotting was generated.</w:t></w:r></w:p><w:bookmarkEnd w:id="268" /><w:bookmarkStart w:id="274" w:name="two-dimenstional-views" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Two-Dimenstional Views</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">While we could consider the Svalbard time series plot as 2-dimensional in the sense that we have two axes defined upon which we plotted the data, with</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">geospatial data</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">we usually think of a 2D plot as one where both axes represent a dimension of space or time. When both axes are defined according to a spatial dimension (e.g, latitude and longitude, easting and northing,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>x</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><m:oMath><m:r><m:t>y</m:t></m:r></m:oMath><w:r><w:t xml:space="preserve">) we call this a map. While maps comprise a lot of different map elements which aid in understanding - in its purest form we have a graphic visualization of a spatial distribution. In digital geovisualziation, different tools are used for mapping. Some of the online maps we have looked at already (e.g., leaflet maps). Lets revisit weather data but move to a more familiar locale. Since we are going to be</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">mapping</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">geospatial data in a 2D geovisualization, we probably want more than one station of data (i.e., a single point on a map doesn’t tell you much about spatial variation - which is typically what we are interested in when mapping).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#CA = stations_ogimet(country =&quot;Canada&quot;, add_map = TRUE, date = as.Date(&quot;2020/01/25&quot;, &quot;%Y/%m/%d&quot;))</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">CA =</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">read.csv</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve">&#39;CA.csv&#39;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">)</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">p &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">ggplot</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(CA, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">aes</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(</w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">x=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">lon, </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DataTypeTok" /></w:rPr><w:t xml:space="preserve">y=</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">lat)) </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">+</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">geom_point</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">()</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">ggplotly</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(p) </w:t></w:r></w:p><w:bookmarkStart w:id="269" w:name="htmlwidget-26f8012ea1d23c35f5e6" /><w:bookmarkEnd w:id="269" /><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Figure 33: Meteorological stations in Canada, data source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId270"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">http://www.ogimet.com/home.phtml.en</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Looking at this map we can see (hopefully) this is indeed Canadian data given the shape of the distribution of points (representing met stations). Two points standout as potential errors we might want to investigate further. Mousing over one at the bottom and in the middle we see that</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">lon = -96.61668 lat = 43.73335</w:t></w:r><w:r><w:t xml:space="preserve">. If we query the data for this we can find out which station this is;</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#which have a latitude less than 45 and a longitude less than -85</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">CA[</w:t></w:r><w:r><w:rPr><w:rStyle w:val="KeywordTok" /></w:rPr><w:t xml:space="preserve">which</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">(CA</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">lat </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">&lt;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">45</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">&amp;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">CA</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">lon </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">&lt;</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">-85</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">),]</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">##       X wmo_id            station_names       lon      lat alt</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="VerbatimChar" /></w:rPr><w:t xml:space="preserve">## 130 130  71168 Sioux Falls Climate, Sd  -96.61668 43.73335 481</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Which we can see is indeed a station in Sioux Falls. So we will drop it from further analysis</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="SourceCode" /></w:pPr><w:r><w:rPr><w:rStyle w:val="CommentTok" /></w:rPr><w:t xml:space="preserve">#drop the row with the station from the US</w:t></w:r><w:r><w:br /></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">CA &lt;-</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">CA[</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">!</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">CA</w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">$</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">wmo_id </w:t></w:r><w:r><w:rPr><w:rStyle w:val="OperatorTok" /></w:rPr><w:t xml:space="preserve">==</w:t></w:r><w:r><w:rPr><w:rStyle w:val="StringTok" /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:rStyle w:val="DecValTok" /></w:rPr><w:t xml:space="preserve">71168</w:t></w:r><w:r><w:rPr><w:rStyle w:val="NormalTok" /></w:rPr><w:t xml:space="preserve">,]</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">There are countless wasy to visualize the spatial distribution meteorlogical stations. We could simply map their coordinates as we have done above, however in some areas like Southwest Ontario where we have lots of points, it is hard to see as so many point symbols are overlapping. We could instead focus the map in a particular region, but then again we would fail to see the broader national-scale picture. Just as we used a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">smoother</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">to visualize the broad scale variation in the time series plot above, we can take the same approach with the spatial data, and map the spatial density of the point distribution. There are again many ways to map a spatial density, and if you are interested in learning more about these techniques you might want to explore classes in spatial analysis (e.g.,</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId271"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">GG361 at Laurier for example</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">).</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">map</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4267200" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 34: Meteorological station as 2D density map in Canada, data source: http://www.ogimet.com/home.phtml.en" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="bookdown-demo_files/figure-docx/unnamed-chunk-59-1.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId272" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4267200" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 34: Meteorological station as 2D density map in Canada, data source:</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId270"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">http://www.ogimet.com/home.phtml.en</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Here we have visualized the spatial distribution usign a density surface mapping method. The details of how this spatial smoother are not important now, but you have probably seen these sort of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">heat maps</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">before, where the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">heat</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">or colour ramp corresponds to the spatial density of the point features being mapped. These heat maps often provide a compelling visualization but</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId273"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">may not tell us anything we don’t already know about the spatial pattern</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">.Here we can see the by far the most met stations are concentrated in the Prairies, the biggest agricultural region of the country. Other</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">hotspots</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">include southern Ontario, Vancouver, and Montreal.</w:t></w:r></w:p><w:bookmarkEnd w:id="274" /><w:bookmarkStart w:id="276" w:name="three-dimensional-views" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Three-Dimensional Views</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">A 3D visualization environment for geospatial data is required for true Digital Earth exploration. As we have covered previously, Google Earth is the most widely used GE application in the world today. Google Earth provides ready access to 3D content integrated with high resolution satellite imagery and the ability to load your own data. You should definitely explore Google Earth and get familiar with how to use it. Here are some stunning examples of 3D visualization using Google Earth.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Here is a sample of global flight data, showing the top 10 destinations and their inbound and outbound flights. With an earth-based visualization we can see global connections not otherwise apparent on a 2D map.</w:t></w:r></w:p><w:bookmarkStart w:id="275" w:name="g31XCcMKge" /><w:bookmarkEnd w:id="275" /><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Figure 35: Global flights data visualizaed on an interactive 3D Globe</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">While many GIS programs allow you to view topographic data, this is often not</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">true</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">3D data. True 3D geospatial data have full access to all three spatial dimensions (x, y, z). Here is a sample of geovisualization of geological data which requires fulll 3D representaton. Whereas a 2D raster map is made up of a grid of pixels, a 3D map is made up of a volume of cells called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">voxels</w:t></w:r><w:r><w:t xml:space="preserve">.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">The video above is a demo from a GIS software company in summer of 2020. The addition of full 3D geovisualizaiton capability to Digital Earth tools is a relatively recent phenoemna and promises to be a big change in how we work with geospatial data. All of the examples here have all three dimensions encoding space, but we can also 2D space + 1D time as a geovisualization approach. This satellite-based high resolution imagign</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">video</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">would be an example of this type of data.</w:t></w:r></w:p><w:bookmarkEnd w:id="276" /><w:bookmarkStart w:id="277" w:name="four-dimensional-views" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Four-Dimensional Views</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">All of the complexity in 3D data plus change-over-time gives us 4D data. For now, we do not really have 4D geovisualization capability in current Digital Earth tools.</w:t></w:r></w:p><w:bookmarkEnd w:id="277" /><w:bookmarkStart w:id="279" w:name="dashboards-views-into-the-digital-earth" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Dashboards Views into the Digital Earth</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Geospatial dashboards, once a fairly rare corner of the GIS world favoured by government planners and corporate managers involved in large-scale planning, have gone mainstream during COVID-19. Virtually every public health authority, department of health, or centre for disease control has created some sort of online dashboard for reporting data on COVID-19. A dashboard is a single online interface that includes multiple views of data, usually 1D time series plots, thematic bart plots, and at a 2D map for visualization of geographic data. The most famous COVID-19 Dashboard is the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId53"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">John Hopkins University COVID-19 Dashboard</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve">. Take some time to explore the interface (this dashboard was built with GIS software).</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="2793795" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 36: Screenshot of the interface of the John Hopkins University COVID-19 Dashboard" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/dashboard.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId278" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="2793795" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 36: Screenshot of the interface of the John Hopkins University COVID-19 Dashboard</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">There are some key features of the dashboard we can highlight. Try clicking on a country name and you will notice that the map pans and highlights the country, and the epi curve plot is upated with that countries data. Filtering data through linked visualizations is called linking, while highlighting specific data points or subsets through linked visualizations is called brushing.</w:t></w:r></w:p><w:bookmarkEnd w:id="279" /><w:bookmarkStart w:id="280" w:name="notes-question" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Notes Question</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Zoom into Canada a bit and examine the subnational data. Do you notice anything unusual about where the points are located? Can you see any issue with this representation?</w:t></w:r></w:p><w:bookmarkEnd w:id="280" /><w:bookmarkEnd w:id="281" /><w:bookmarkStart w:id="292" w:name="earth-system-geovisualization" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Earth System GeoVisualization</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">While the examples we looked above give us an idea of what sorts of tools and approaches are available for geovisualization, we know that the value of the Digital Earth really accelerates when we bring together multiple types of data together. We will examine some existing geovisualization platforms for earth systems data, tools that are essential to understand our changing planet.</w:t></w:r></w:p><w:bookmarkStart w:id="284" w:name="noaa-view-data-exploration-tool" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:hyperlink r:id="rId282"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">NOAA View Data Exploration Tool</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Description</w:t></w:r><w:r><w:t xml:space="preserve">: The NOAA View Data Exploration Tool is a webmap interface to earth systems data for the entire globe. This includes data describing Oceans, Atmosphere, Land, Cryosphere (i.e., snow/ice), climate, and weather forecast models. Here we show how to use the model data to forecast snowfall up to 16 days in the future.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Datasets on the NOAA tool have rich metadata available to learn more about what the data represent. The geovisualization capabilities in the tool are fairly standard, allowing 2D exploration of the data through panning and zooming and a map-animation tool for view change-over-time. The resolution of the data is fairly coarse; focused on global and large-scale regional patterns of key earth processes.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3331763" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 37: Screenshot of the interface of NOAA View Data Exploration Tool showing metadata for the Active Fires dataset" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/daa.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId283" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3331763" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 37: Screenshot of the interface of NOAA View Data Exploration Tool showing metadata for the Active Fires dataset</w:t></w:r></w:p><w:bookmarkEnd w:id="284" /><w:bookmarkStart w:id="288" w:name="global-climate-dashboard" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:hyperlink r:id="rId285"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Global Climate Dashboard</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Description</w:t></w:r><w:r><w:t xml:space="preserve">: The Global Climate Dashboard created and hosted by NOAA provides an accessible tool for global climate change data. The focus here is on communicating results of climate science rather than as a portal to access raw data. The thematic foci is observed changes in climate, climate variability, and climate model projections. The visualization interface is 1D only, showing change over time in three user-selected variables.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Viewing the data on Spring Snow Cover (million km</w:t></w:r><m:oMath><m:sSup><m:e><m:r><m:t>&#8203;</m:t></m:r></m:e><m:sup><m:r><m:t>2</m:t></m:r></m:sup></m:sSup></m:oMath><w:r><w:t xml:space="preserve">) shows how observed snow cover compares to the longterm average. Clicking on the</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId286"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">learn more</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">link brings up a detailed report of an analysis of changing spring snow cover, including details on how snow cover is measured.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="4345617" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 38: Screenshot of the linked report based on climate.gov data on long-term changes in observed global spring snow cover" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/snow-cover.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId287" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="4345617" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 38: Screenshot of the linked report based on climate.gov data on long-term changes in observed global spring snow cover</w:t></w:r></w:p><w:bookmarkEnd w:id="288" /><w:bookmarkStart w:id="291" w:name="arctic-biodiversity-data-service" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:hyperlink r:id="rId289"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Arctic Biodiversity Data Service</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The Arctic Biodiversity Data Service is a set of Digital Earth tools created and managed by the Conservation of Arctic Flora and Fauna, which is the biodiversity working group of the Arctic Council. The ABDS has limited visualization of the data layers themselves but functions as more of a data-catalog, providing access to curated datasets on processes and indicators important for monitoring change in the Arctic.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="3673415" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 39: Screenshot of a dataset from the Arctic Biodiversity Data Service" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/abds.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId290" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="3673415" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 39: Screenshot of a dataset from the Arctic Biodiversity Data Service</w:t></w:r></w:p><w:bookmarkEnd w:id="291" /><w:bookmarkEnd w:id="292" /><w:bookmarkStart w:id="294" w:name="story-maps-digital-earth-storytelling" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Story Maps: Digital Earth Storytelling</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The views of the Digital Earth we have explored are all designed to facilitate</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">data exploration</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">and in some cases</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">data discover and data access</w:t></w:r><w:r><w:t xml:space="preserve">. In the Global Climate Dashboard tool we saw links to external reports that provided a contextualized analysis of the data, highlighting important changes based on scientific study. This need for more interpretation of raw data, richer and more narrative</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">story-telling</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">approaches with Digital Earth data have also emerged in recent years. One set of tools for creating this narrative is called a</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">Story Map</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">which is a 2D webmap that has linked views and mixed of geospatial content, images, text and even video and sound. As maps, these other forms of content are anchored to their related geographies through the map, and the reader can navigate to pre-specified locations in the map and see information within their geographic context. As such story maps have been used in both natural science and social science applications of the Digital Earth.</w:t></w:r></w:p><w:bookmarkStart w:id="293" w:name="X11e35722ae79da56cefd05346df3211636b5e7c" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Case Study: City of Kitchener Lifestyle Segments</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The City of Kitchener has launched several story maps, and one in particular provides a good example of the idea of mixing media types with a web map interface. The PRIZM5 Lifestyle Segments categorizes Canada’s population into 68 lifestyle types which captures demographics, lifestyles and consumer behaviours methodology. This sort of segmentation analysis can be useful for planning government services and for companies to know about differences in local markets.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Explore the City of Kitchener PRIZM5 Lifestyle Segments story map. In particular explore the Top 10 Lifestyle Segments which links the segment type with the spatial distribution of people classified in that segment.</w:t></w:r></w:p><w:bookmarkEnd w:id="293" /><w:bookmarkEnd w:id="294" /><w:bookmarkStart w:id="297" w:name="the-power-and-limits-of-geovisualization" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">The Power and Limits of GeoVisualization</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">The UCGIS paper we linked to at the beginning of the topic introduced a conceptual model for geovisualization introduced by Alan McEachern, called Cartography</w:t></w:r><m:oMath><m:sSup><m:e><m:r><m:t>&#8203;</m:t></m:r></m:e><m:sup><m:r><m:t>3</m:t></m:r></m:sup></m:sSup></m:oMath><w:r><w:t xml:space="preserve">. The model links together three dimensions upon which we can evlaute a geovisualization tool.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="CaptionedFigure" /></w:pPr><w:r><w:drawing><wp:inline><wp:extent cx="5334000" cy="5271561" /><wp:effectExtent b="0" l="0" r="0" t="0" /><wp:docPr descr="Figure 40: The Cartography3 model of geovisualization" title="" id="1" name="Picture" /><a:graphic><a:graphicData uri="http://schemas.openxmlformats.org/drawingml/2006/picture"><pic:pic><pic:nvPicPr><pic:cNvPr descr="images/geovizmodel.png" id="0" name="Picture" /><pic:cNvPicPr><a:picLocks noChangeArrowheads="1" noChangeAspect="1" /></pic:cNvPicPr></pic:nvPicPr><pic:blipFill><a:blip r:embed="rId295" /><a:stretch><a:fillRect /></a:stretch></pic:blipFill><pic:spPr bwMode="auto"><a:xfrm><a:off x="0" y="0" /><a:ext cx="5334000" cy="5271561" /></a:xfrm><a:prstGeom prst="rect"><a:avLst /></a:prstGeom><a:noFill /><a:ln w="9525"><a:noFill /><a:headEnd /><a:tailEnd /></a:ln></pic:spPr></pic:pic></a:graphicData></a:graphic></wp:inline></w:drawing></w:r></w:p><w:p><w:pPr><w:pStyle w:val="ImageCaption" /></w:pPr><w:r><w:t xml:space="preserve">Figure 40: The Cartography3 model of geovisualization</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">On one axes is the task complexity; which varies from simple information sharing to knowledge construction. As the field of</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">geovisual analytics</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">has emerged in recent years, more geovisualization tools are capable of deriving new and original insights by working directly with data. On another axis is the degree of interaction in terms of how much the user can actively engage with data in the geovisualization tool. Typically, higher interaction is associated with higher task complexity - so more complex knowledge construction tasks require more input from the user. The remaining (vertical) axis represents the expertise of users, from specialists to the general public. The line in the middle of the cube represents the transition from high task complexity, high user interaction, specialist user knoweldge geovisualization tools, to tools with lower iteraction, requiring less specialist knowledge, and more for sharing existing information. Each of the tools we have reviewed above can be categorized accordign to these three core concepts.</w:t></w:r></w:p><w:bookmarkStart w:id="296" w:name="notes-question-1" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Notes Question</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">Using the Cartography</w:t></w:r><m:oMath><m:sSup><m:e><m:r><m:t>&#8203;</m:t></m:r></m:e><m:sup><m:r><m:t>3</m:t></m:r></m:sup></m:sSup></m:oMath><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:b /></w:rPr><w:t xml:space="preserve">framework above, categorize one of the geovisualization tools we have reviewed this week. Provide a sentence describing why you rated it the way you did for each the three dimensions.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">Despite the obvious power of viewing and interacting with Digital Earth data through geovisualization tools, there are limits to what can be done through visualization alone. Complex analytical tasks in machine learning and statistical modelling can be facilitated through visualization, but ultimately depend on computational models for learning relationships from data.</w:t></w:r></w:p><w:bookmarkEnd w:id="296" /><w:bookmarkEnd w:id="297" /><w:bookmarkStart w:id="299" w:name="summary" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Summary</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">We have seen that there are many ways to view and explore the Digital Earth. We have viewed multiple dimensions of views of geospatial and temporal data, as well how story maps can enable personal narratives and rich media attached to geospatial data. A conceptual model for evaluating and categorizing geovisualization tools was introduced. The future may require more innovating approaches to visualization of 4D geospatial data.</w:t></w:r></w:p><w:bookmarkStart w:id="298" w:name="key-terms-from-this-week" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Key terms from this week</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">geovisualization</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">4D visualization</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Google Earth</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">time series</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">spatial time series</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">story maps</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1041" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">dashboard</w:t></w:r></w:p><w:bookmarkEnd w:id="298" /><w:bookmarkEnd w:id="299" /><w:bookmarkEnd w:id="300" /><w:bookmarkStart w:id="304" w:name="X8e0d84b3e769d8c204da064ef90d0f9e02a5bd5" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">Term Project and Digital Earth Project Management</w:t></w:r></w:p><w:bookmarkStart w:id="303" w:name="preliminaries" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Preliminaries</w:t></w:r></w:p><w:bookmarkStart w:id="301" w:name="readings" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Readings</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Readings should be done when referenced during this week’s lesson. Do not read before starting the lesson.</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1042" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">x1</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1042" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">x2</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1042" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">x3</w:t></w:r></w:p><w:bookmarkEnd w:id="301" /><w:bookmarkStart w:id="302" w:name="learning-objectives" /><w:p><w:pPr><w:pStyle w:val="Heading3" /></w:pPr><w:r><w:t xml:space="preserve">Learning Objectives</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">By the end of this lesson, students will be able to:</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1043" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">d1</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1043" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">s2</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1043" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">d3</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1043" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">d4</w:t></w:r></w:p><w:bookmarkEnd w:id="302" /><w:bookmarkEnd w:id="303" /><w:bookmarkEnd w:id="304" /><w:bookmarkStart w:id="317" w:name="the-digital-earth-town-hall" /><w:p><w:pPr><w:pStyle w:val="Heading1" /></w:pPr><w:r><w:t xml:space="preserve">The Digital Earth Town Hall</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">During the week of December 9th 2020, we will convene a Digital Earth Town Hall with guest speakers, a discussion panel, and a map contest.</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="BodyText" /></w:pPr><w:r><w:t xml:space="preserve">`r if (knitr:::is_html_output()) ’</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve"># Assignments {-}</w:t></w:r></w:p><w:bookmarkStart w:id="305" w:name="module-1-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 1 Assignment</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1044" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:r><w:t xml:space="preserve">Introduce yourself on the discussion board. Write a brief paragraph about why you</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:rPr><w:i /></w:rPr><w:t xml:space="preserve">might</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">be interested in learning about the Digital Earth. That’s it! Just get to know a few people in the course and letting the instructor get to know some of your backgrounds and interests is all that is required for Module 1.</w:t></w:r></w:p><w:bookmarkEnd w:id="305" /><w:bookmarkStart w:id="310" w:name="module-2-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 2 Assignment</w:t></w:r></w:p><w:p><w:pPr><w:pStyle w:val="FirstParagraph" /></w:pPr><w:r><w:t xml:space="preserve">Google Earth is currently the most widely used Digital Earth application. A tool built using Google Earth called</w:t></w:r><w:r><w:t xml:space="preserve"> </w:t></w:r><w:hyperlink r:id="rId306"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Voyager</w:t></w:r></w:hyperlink><w:r><w:t xml:space="preserve"> </w:t></w:r><w:r><w:t xml:space="preserve">provides a series of pre-defined guided tours, quizzes, and layers that focus on particular themes of issues around the globe. You are going to explore three of these stories</w:t></w:r></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1045" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId307"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Frozen Lakes</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1045" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId308"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Mapping London’s Air Pollution</w:t></w:r></w:hyperlink></w:p><w:p><w:pPr><w:numPr><w:ilvl w:val="0" /><w:numId w:val="1045" /></w:numPr><w:pStyle w:val="Compact" /></w:pPr><w:hyperlink r:id="rId309"><w:r><w:rPr><w:rStyle w:val="Hyperlink" /></w:rPr><w:t xml:space="preserve">Underwater Earth Imagery</w:t></w:r></w:hyperlink></w:p><w:bookmarkEnd w:id="310" /><w:bookmarkStart w:id="311" w:name="module-3-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 3 Assignment</w:t></w:r></w:p><w:bookmarkEnd w:id="311" /><w:bookmarkStart w:id="312" w:name="module-4-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 4 Assignment</w:t></w:r></w:p><w:bookmarkEnd w:id="312" /><w:bookmarkStart w:id="313" w:name="module-5-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 5 Assignment</w:t></w:r></w:p><w:bookmarkEnd w:id="313" /><w:bookmarkStart w:id="314" w:name="module-6-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 6 Assignment</w:t></w:r></w:p><w:bookmarkEnd w:id="314" /><w:bookmarkStart w:id="315" w:name="module-7-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 7 Assignment</w:t></w:r></w:p><w:bookmarkEnd w:id="315" /><w:bookmarkStart w:id="316" w:name="module-8-assignment" /><w:p><w:pPr><w:pStyle w:val="Heading2" /></w:pPr><w:r><w:t xml:space="preserve">Module 8 Assignment</w:t></w:r></w:p><w:bookmarkEnd w:id="316" /><w:bookmarkEnd w:id="317" /><w:sectPr /></w:body></w:document>